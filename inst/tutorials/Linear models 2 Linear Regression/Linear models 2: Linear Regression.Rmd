---
title: "Linear models 2: Linear Regression"
output: 
  learnr::tutorial:
    progressive: false
    theme: cerulean
    highlight: pygments
    css: css/test.css
    code_folding: hide
runtime: shiny_prerendered
author: Rob Knell
description: >
  Linear models with a single, continuous explanatory variable. How to fit a straight line through a cloud of data, partitioning the variance for a significance test and how to interpret the output.
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = TRUE, comment = NA)


```


## Fitting lines to bivariate data

We very often find ourselves with sets of *bivariate data*: two separate variables have been measured for each thing we're interested in. This might be genetic variability and distance from another population, it might be nutrient intake and immune response, it might be height on a mountain and the size of a plant, or it might be any number of combinations of variables. Quite often in these circumstances we are interested in the effect that one variable (often called the *independent* variable) has on another (the *dependent* variable). Sometimes we find complex relationshpis between variables wthat have to be described by some form of curve, but sometimes we are lucky and there is a simple relationship between the two which we can describe with a straight line. In this case we can use a type of linear model where instead of relating our response variable to an explanatory variable which is a factor with discrete levels, as you saw when we were looking at ANOVA, we use a continuous explanatory variable and we describe the relationship in terms of the slope and the intercept of the straight line which best describes how the dependent variable changes with the independent variable. This form of analysis is often called *linear regression* and the basics are explained in this video:

VIDEO

Let's work though an example of a linear regression. The `Latitude_diversity` data set contains data on tree diversity from 24 forest plots in locations ranging from the tropics to northern Europe and the USA^1^. One of the measures we have for each plot is the Shannon diversity index, a measure of the overall diversity of trees present, and another is the latitude of the plot. Let's have a look at these data with a scatterplot. 
We'll use the absolute value of latitude because some values are negative (the plot is South of the Equator) but the pattern we're interested in is how diversity changes with distance from the equator.

```{r fig.cap = "**Figure 1** Shannon diversity for trees plotted against latitude for 24 forest plots"}

diversity <- read.csv("https://github.com/rjknell/Linear_models/raw/master/data/Latitude_diversity.csv")

plot(Shannon_diversity ~ abs(Latitude),
            data = diversity,
            pch = 16,
            col = "aquamarine4",
            ylab = "Shannon diversity",
            xlab = "Absolute latitude")
```
As you can see the Shannon diversity index decreases with increasing latitude, and just from looking at these data we can see that they might be suitable for linear regression analysis. We have a clear potential direction of causality (diversity does not cause latitude but latitude might cause diversity), the relationship is not obviously different from a straight line and the data don't have any obvioius skew, outliers or other wierdness.

If we want to find the line that best allows us to predict the Shannon diversity of a patch of forest from its latitude, we can fit a linear regression using the `lm()` (for linear model) function with a *formula* specifying exactly what we want the function to fit, much as we've seen for ANOVA and also for plotting data --- in fact the formula we'll use with `lm()` here is exactly the same as the one we used to generate the plot. We have the *response variable* (AKA the dependent variable), then a *tilde* (~), the symbol which means "as explained by" in an R formula, and then the *explanatory variable*, AKA the independent variable. If both are in the same data frame we can use the `data = ` argument to tell R where to look. To fit a linear model to these data, therefore we can use this code:

```{r}
L1 <- lm(Shannon_diversity ~ abs(Latitude), data = diversity)

L1
```
Just inputting the name of the fitted `lm()` object returns the formula and the model coefficients. The first one, labelled *Intercept* is the y-intercept, or the predicted value when the explanatory variable is zero, which in this case would be the predicted Shannon diversity at the equator, which is 4.82. The second one, labelled with the name of the *explanatory variable* is the slope of the relationship, or in other words the amount by which we predict the Shannons diversity index to change with every degree of increase in latitude. Putting these into the equation of a straight line we get:

$$ y = -0.0741 \times |latitude| + 4.82$$
and we can draw this onto our scatterplot using the `abline()` function.

```{r fig.cap = "**Figure 1** Shannon diversity for trees plotted against latitude for 24 forest plots with a line fitted from a linear regression"}

plot(Shannon_diversity ~ abs(Latitude),
            data = diversity,
            pch = 16,
            col = "aquamarine4",
            ylab = "Shannon diversity",
            xlab = "Absolute latitude")

abline(
  a = 4.82,
  b = -0.0741,
  lwd = 1.5,
  col = "aquamarine4"
)
```

Is the line a good fit to the data? It's not perfect of course because there's a fair amount of noise in these data but just from looking at it we can see that it's going to be difficult to find anything better. Is the line statistically significant? As we saw in the video, fitting a linear regression involves partitioning variance in a very similar way to what happens with an ANOVA, and just like an ANOVA we can carry out an F-test to see if the amount of variance explained by our fitted model (the treatment variance or the treatment MS) is greater than that which is unexplained (the error variance or MS error, or as R labels it the residual variance). Using the `anova()` function on our model will generate an ANOVA table just like the one we saw when we looked at single factor ANOVA.

```{r}
anova(L1)
```
This is calculated in the same way as the ANOVA table for a single factor ANOVA, except that instead of calculating the SS error (or the Residual Sum of Squares as R calls it) by subtracting a group mean from each data point we subtract the predicted value of the fitted line: so if we have a data point which has a latitude of 9.15 and a Shannon diversity of 4.0 (This is Barro Colorado Island, Panama in our data) the predicted value is $4.82 - 0.0741 \times 9.15 = 4.142$. $(x - \bar{x})^2$ is therefore $\left( 4.0 - 4.142 \right)^2 = 0.020$ for this particular data point. Just to make the point we can calculate our sums of squares separately.

```{r}
SSTotal <- sum((diversity$Shannon_diversity - 
                  mean(diversity$Shannon_diversity))^2)

SSError <- sum((diversity$Shannon_diversity - 
                  (4.82 - 0.0741 * abs(diversity$Latitude)))^2)
               
SSTreatment <- SSTotal - SSError

cat("SSTreatment = ",SSTreatment)
cat("SSError = ",SSError)
```

Compare these to the ANOVA table above and you can see that they match the entries in the Sum Sq column. The Mean square values are just the Sums of Squares divided by the degrees of freedom and the F statistic is the Mean square treatment (labelled as `abs(Latitude)` here) divided by the Mean Square error (labelled `Residuals` here).

In practice we don't need to calculate our sums of squares separately, nor do we even need to use `anova()` to get an ANOVA table for our linear regression. If we use `summary()` on our fitted model object we can get all the information we need.

```{r}
summary(L1)
```
The model summary should be somewhat familiar to you from the ANOVA tutorial. Here it follows the same layout, so we have a reminder of what the model formula is, some summary statistics for the *residuals*, then the table of coefficients. This table has the estimate for the intercept and the slope of the line, their standard errors and some p-values derived from marginal t-tests. The p-value for both the intercept and the slope tell us in this case that they are both significantly different from zero. That's not especially useful for the intercept except in some particular cases, but it is of course an important hypothesis test for the slope: the null hypothesis for most linear regressions is that the slope is equal to zero. If you've ever been taught to test for the significance of a regression using a t-test this is the equivalent of what you were taught previously.

After the coefficients table there are some summary statistics for our model fit. The most commonly used one of these is the R-squared value (here written as "Multiple R-squared") which is telling us the proportion of the overall variation which is explained by our model. This is the same as the R-squared value that you might have enountered in correlation analysis and we can derive it ourselves from the SSTreatment and SSTotal which we calculated earlier:

```{r}
SSTreatment / SSTotal
```

Which is the same value as the one in the table. So overall, latitude explains 63% of the overall variaotin in diversity. In ecology at least that's a high value for r-squared and indicates that the relationship between latitude and diversity is a strong one. The adjusted R-squared value is adjusted by the number of explanatory variables in the model and we can ignore it because a) we've only got one explanatory variable and b) no-one uses it anyway.

The last line of the output from `summary()` gives us the same F-statistic and test that we had form our ANOVA table. You might notice that the p-value from the t-test in the coefficients table is in fact exactly the same as the one for the F-test based on partitioning the variance. This is the case for linear regression with a single explanatory variable only so be a little careful about the marginal value from the coefficients table: yes in this case it's also equivalent to a significance test for the whole model, but don't think it is when you're dealing with more complex statistical models with multiple explanatory variables. It's quite common for people to get a bit confused over this so best to make it clear now.
<br><br><hr>

1: Originally published in LaManna, J.A., Mangan, S.A., Alonso, A., Bourg, N.A., Brockelman, W.Y., Bunyavejchewin, S., Chang, L.-W., Chiang, J.-M., Chuyong, G.B., Clay, K., Condit, R., Cordell, S., Davies, S.J., Furniss, T.J., Giardina, C.P., Gunatilleke, I.A.U.N., Gunatilleke, C.V.S., He, F., Howe, R.W., Hubbell, S.P., Hsieh, C.-F., Inman-Narahari, F.M., Janík, D., Johnson, D.J., Kenfack, D., Korte, L., Král, K., Larson, A.J., Lutz, J.A., McMahon, S.M., McShea, W.J., Memiaghe, H.R., Nathalang, A., Novotny, V., Ong, P.S., Orwig, D.A., Ostertag, R., Parker, G.G., Phillips, R.P., Sack, L., Sun, I.-F., Tello, J.S., Thomas, D.W., Turner, B.L., Vela Díaz, D.M., Vrška, T., Weiblen, G.D., Wolf, A., Yap, S. & Myers, J.A. (2017) Plant diversity increases with the strength of negative density dependence at the global scale. Science, 356, 1389–1392.

## Linear regression exercise: analysing mammal abundance near villages in Gabon

In 2017, Sally Koerner and co-authors^1^ published a study of how animal abundance changes with distance from villages in Gabon. This involved establishing 24 2.5km transects at varying distances from the nearest village, monitoring them monthly and recording all of the mammals and birds encountered. We'll look at a small subset of their data, focussing on how the relative abundance of ungulates and rodents changed with the distance from the nearest village. Relative abundance was calculated for each group of animals as the percentage of all encounters on that transect which were with that particular group.

The 




1: Koerner, S.E., Poulsen, J.R., Blanchard, E.J., Okouyi, J. & Clark, C.J. (2017) Vertebrate community composition and diversity declines along a defaunation gradient radiating from rural villages in Gabon. The Journal of applied ecology, 54, 805–814.
