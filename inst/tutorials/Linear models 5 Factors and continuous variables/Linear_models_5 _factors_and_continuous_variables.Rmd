---
title: "Linear models 5: Factors and continuous variables"
output: 
  learnr::tutorial:
    progressive: false
    theme: cerulean
    highlight: textmate
    css: css/test.css
    code_folding: hide
runtime: shiny_prerendered
author: Rob Knell
description: >
  Linear models with both factors and continuous variables as explaantory variables: how to fit models, check diagnostics and interpret the output.
---


```{r setup, include=FALSE}
library(learnr)

library(RColorBrewer)

knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE, fig.width = 5, fig.height = 5)

pinniped <- read.csv("https://github.com/rjknell/Linear_models/raw/master/data/Pinniped_brains.csv")

pinniped$Mate_type <- as.factor(pinniped$Mate_type)

beetle <- read.csv("https://github.com/rjknell/Linear_models/raw/master/data/Okada2019.csv")

beetle$treatment <- as.factor(beetle$treatment)

palette1 <- brewer.pal(n = 6, name = 'Dark2')
```

## Models combining factors and continuous explanatory variables

You should now be starting to feel familiar with fitting linear models with multiple explanatory variables. Up to this point, however, we've focussed on either multiple factors (tutorial 3) or multiple continuous variables (tutorial 4). The next step, of course, is to fit models with both factors and continuous variables. People often find these difficult to interpret, but the trick is to remember that a model with a factor and a continuous explanatory variable is essentially fitting a series of lines, one for each factor level, with the slope being determined by the relationship between the continuous variable and the response variable. If there is no interaction then the lines will all share a common slope, with the differences in intercept being determined by the factor levels. An interaction term in a model with a factor and a continuous variable means that the lines are fitted with a different slope for each level of the factor. 


## Exercise 1: pinniped brains and mating systems

 Let's start with an fairly straightforward example. This comes from a study of brain size and mating type in pinnipeds (seals, sealions and walruses) published in 2012 by John Fitzpatrick and coauthors^1^. As part of a study of how mating system might impact on the evolution of brain and body size, Fitzpatrick *et al.* collected data on brain and body size and mating system for males and females from 33 species of pinniped. The data are available on github as a .csv file at https://github.com/rjknell/Linear_models/raw/master/data/Pinniped_brains.csv. Start by importing the data and saving it as an object called `pinnipeds`, and then checking the import with the `str()` function.

```{r pinniped_import, exercise=TRUE}

```

```{r pinniped_import-hint-1}
# You need to use the read.csv() function
# to import the data.
```

```{r pinniped_import-hint-2}
# Don't forget to put the file path (in this
# case the URL) in quote marks
```

```{r pinniped_import-hint-3}
# This is the solution
pinniped <- read.csv("https://github.com/rjknell/Linear_models/raw/master/data/Pinniped_brains.csv")

str(pinniped)
```

We're interested here in the relationship between body mass and brain mass for males, and how it relates to the intensity of sexual selection each species experiences. The latter is indicated here by a variable called `Mate_type` which has two values. `mono` indicates species where the males are *monogynous*, pairing with a single female at a time, whereas `poly` indicates species with *polygynous* males which defend groups of females against rival males during the breeding season. Polygynous males are expected to experience stronger sexual selection during the breeding season.

<br>
<img src = https://github.com/rjknell/Linear_models/raw/master/inst/tutorials/Linear%20models%205%20Factors%20and%20continuous%20variables/Cape_fur_seals.jpg width = "500">

Two male pinnipeds (Cape Fur Seals *Arctocephalus pusillus*, actually a species of sealion) competing for territory in which to defend a group of females. Photo copyright Rob Knell 2019.

<br>

Before going any further we should do some exploratory analysis. In this case a scatterplot of `Male_brain_g` versus `Male_mass_Kg` would be appropriate, and you can colour-code it for `Mate_type` by firstly declaring `Mate_type` to be a factor and then specifying `col = Mate_type` as an argument in your `plot()` function call. Since we're just doing some exploratory analysis we won't worry about a legend --- because `poly` comes after `mono` in the alphabet the `poly` data will be coded red and the `mono` data black. Don't forget to label the axes appropriately.

```{r pinniped_scatterplot1, exercise = TRUE}


```

```{r pinniped_scatterplot1-hint-1}
# Make `Mate_type` into a factor by using the
# as.factor() function:

pinniped$Mate_type <- as.factor(pinniped$Mate_type)
```

```{r pinniped_scatterplot1-hint-2}
#  This is the solution
pinniped$Mate_type <- as.factor(pinniped$Mate_type)

plot(Male_brain_g ~ Male_mass_Kg, 
     col = Mate_type, 
     data = pinniped,
     xlab = "Male mass (Kg)",
     ylab = "Brain mass (g)")
```

<br><br>
What do you conclude from this plot?

```{r pinniped_scatterplot_quiz, echo=FALSE}


question("Based on the scatterplot above, which of the following do you think are correct?",
  answer("The relationship between brain mass and body mass is curved and it will be necessary to use a quadratic term in our model", message  = "Answer 1: It's hard to be sure about the nature of this relationship because of the strong positive skew in both variables. It is possible that any curvature will be removed if an appropriate transformation is used. Although there does seem to be some curvature, therefore, it's not clear that we need to fit a curved model."),
  answer("Both variables show strong negative skew", message = "Answer 2. No, these data are showing positive skew. Negative skew would show as the opposite pattern, with data points clustered towards the higher values."),
  answer("There are a number of data points with extreme values which should be removed as outliers", message = "Answer 3. Without any other good reason (data point known to be problematic for reasons other than its value), outliers should only be cautiously removed if the are clearly anomalous and not within the range of values that might be expected given the distribution of the data. Given the strong positive skew evident here there is no reason to remove these extreme values"),
  answer("Both variables show strong positive skew and a data transformation such as a log transformation will be necessary before further analysis", correct = TRUE),
  answer("The heaviest species are all polygynous so the analysis will be invalid unless we remove these", message = "There's no requirement that the explanatory variable should have the same range for both groups. Nonetheless, we should bear this in mind in the analysis: for each mating type we can only really draw conclusions regarding the range of data that we have represented") 
)
```

<br>
<details><summary>**Click here for more on the exploratory analysis**</summary>
<br>
Both variables are strongly positively skewed and it's going to be necessary to correct this with a transformation before proceeding further. Given the nature of the data and the strong positive skew a log transformation would be appropriate: try to generate a new plot like the previous one but with both variables logged. NB neither variable has any zeros so no need to add a constant.


```{r pinniped_scatterplot2, exercise = TRUE}
plot(Male_brain_g ~ Male_mass_Kg, 
     col = Mate_type, 
     data = pinniped)
```

```{r pinniped_scatterplot2-hint-1}
# Just use the log() function on each variable.
# Alternatively, use the log = "xy" argument 

```

```{r pinniped_scatterplot2-hint-2}
#  This is the solution
#  
plot(log(Male_brain_g) ~ log(Male_mass_Kg), 
     col = Mate_type, 
     data = pinniped,
     xlab = "Log of male mass (Kg)",
     ylab = "Log of brain mass (g)")
```

The log transformation seems to have sorted out the problems associated with the very skewed data. Now to fit a model. We'd like the main effects of both `Male_mass_Kg` and `Mate_type`, plus their interaction. Don't forget to log both of the variables that require transformation, and call your fitted model object `P1`.

Once the model is fitted, check the significance of the interaction term using `drop1()`.

```{r pinniped_model1, exercise = TRUE}


```

```{r pinniped_model1-hint-1}
# To fit both main effects and the interaction
# you can use var1 + var2 + var1:var2 or
# var1 * var2.

# You need to specify data = pinniped as an argument
```

```{r pinniped_model1-hint-2}
# For the drop1() don't forget to specify
# test = "F" as an argument.

# Make sure all arguments are separated by a comma 
# for both functions
```

```{r pinniped_model1-hint-3}
# Here is the solution

P1 <- lm(log(Male_brain_g) ~ log(Male_mass_Kg) * Mate_type,
         data = pinniped)

drop1(P1, test = "F")
```

A model with the interaction present has higher explanatory power than one with just the main effects so we'll keep the present model.

</details>

<br><br><hr>

1. Fitzpatrick, J.L., Almbro, M., Gonzalez-Voyer, A., Hamada, S., Pennington, C., Scanlan, J. & Kolm, N. (2012) Sexual selection uncouples the evolution of brain and body size in pinnipeds. Journal of evolutionary biology, 25, 1321–1330.

## Diagnostics for the pinniped model

Before we go any further with trying to interpret the model we need to check the diagnostics for our model. Use the `plot()` function and the `which = 1:2` argument to bring up only the first two diagnostic plots.

```{r pinniped_model, echo = FALSE}
P1 <- lm(log(Male_brain_g) ~ log(Male_mass_Kg) * Mate_type,
         data = pinniped)
```


```{r pinniped-diagnostics, exercise.setup = "pinniped_model", exercise = TRUE}

```

```{r pinniped-diagnostics-hint-1}
# This is the solution
plot(P1, which = 1:2)
```

Take a look at these diagnostics and try to answer these questions.

```{r pinniped-diagnostics-quiz, echo=FALSE}
  question(
    "Which of the following statements is correct?",
    answer("The residuals versus fitted values plot shows evidence of strong heteroskedasticity which needs to be corrrected", message = "Answer 1. Heteroskedascticity means that the variance is varying across the range of fitted values, often leading to a plot with a characteristic wedge shape. This plot has some suggestions of a wedge shape but it's not clear and is unlikely to be causing problems with the model fit."),
    answer("It is not clear but there is some indication of negative skew", message = "Answer 2. There's nothing here to indicate negative skew, which would show as more very negative residuals and a convex curve of points on the qq-plot."),
    answer("The qq-plot shows that there is some positive skew in the residuals", message = "Answer 3. The qq-plot is rather well-behaved with all the residuals plotting close to the line. There's nothing to indicate skew in these data."),
    answer(
      "The qq-plot shows that the relationship between at least onf of the explanatory variables and the response variable is non-linear", message = "Answer 4. The qq-plot can only tell you about the distribution of the residuals, not about whether the shape of the relationship between response and epxlanatory variables is linear."),
      answer("The qq-plot indicates that the residuals are approximately normally distributed", correct = TRUE)
    )
```
<br>
<details><summary>**Click here for more on the diagnostics**</summary>

The diagnostic plots do not iondicate anything particularly concerning. The qq-plot shows no serious deviation from what would be expected if the residuals were normally distributed. The residuals versus fitted values plot shows some hint of increasing variance with higher fitted values but this is not clear. When sample sizes are relatively small it becomes more difficult to see real patterns on diagnostic plots. Furthermore, when sample sizes are small we quite often see patterns that are more a consequence of the chance distribution of a few large positive or negative residuals than representative of a real issue with the data. Given the uncertainty about whether there really is heteroskedasticity, and the general robustness of the linear model, we will leave it as it is.
</details>

## Interpretation of the pinniped model

This video deals with model summaries when there are both factors and continuous explanatory variables. You might want to skip the first 6 minutes which are focussed on simple linear regression but if you're still at all confused about coefficients and summary tables I'd recommend watching the whole thing for a bit of revision.

![](https://youtu.be/aCOgHUSd-SI)

Now check the `summary()` output for the model P1.

```{r pinniped_summary1, exercise.setup = "pinniped_model", exercise = TRUE}

```

```{r pinniped_summary1-hint-1}
# This is the solution

summary(P1)
```
Since we have a model with one continuous explanatory variable and one factor with two levels, and we have an interaction term in our model, we are fitting a model that consists of two lines with different slopes as well as different intercepts. On the basis of the explanation given in the video, try to work out that the equations for the two lines relating log male mass to log brain mass are. Just as a reminder the two factor levels in `Mate_type` are `mono` and `poly` and `mono` comes first alphabetically.

```{r pinniped-summary-quiz, echo=FALSE}
quiz(caption = "Pinniped model interpretation quiz",
  question(
    "Which of the following is the equation of the line relating log male mass to log brain size for **monogynous** species?",
    answer("log(Brain mass) = 2.0779 + 1.9273 x log(Male mass)", message = "Answer 1. 1.9273 is the difference in intercepts and not  slope"),
    answer("log(Brain mass) = 2.0779 + 0.7544 x log(Male mass)", correct = TRUE),
    answer("log(Brain mass) = 2.0779 + 1.9273 + 0.7544 x log(Male mass)", message  = "Answer 2. 1.9273 is the difference in intercepts between the two mating systems and should be added on for polygynoous species"),
    answer("log(Brain mass) = 2.0779 + 1.9273 + (0.7544 - 0.3767) x log(Male mass)", message = "Answer 4. The coefficients for the monogynous species are the first two in the coefficients table.")
    ),

question(
    "Which of the following is the equation of the line relating log male mass to log brain size for **polygynous** species?",
    answer("log(Brain mass) = 2.0779 + 1.9273 x log(Male mass)", message = "Answer 1. 1.9273 is the difference in intercepts and not  slope"),
    answer("log(Brain mass) = 2.0779 + 0.7544 x log(Male mass)", message= "This is the equation for monogynous species"),
    answer("log(Brain mass) = 2.0779 + 1.9273 + 0.7544 x log(Male mass)", message  = "Answer 2. Partly right - but you need the difference in slopes as well as the difference in intercepts"),
    answer("log(Brain mass) = 2.0779 + 1.9273 + (0.7544 - 0.3767) x log(Male mass)", correct = TRUE)
    ),

question(
  "Which of the following statements is true?",
  answer("Brain mass increases more with body mass in polygynous species", message = "The slope is lower in polygynous species so brain mass increases more slowly"),
  answer("Brain mass decreases with body mass in polygynous species", message = "Brain mass increases with body mass in polygynous species, but it does so less than in monogynous species"),
  answer("On average, for every Kg increase in body mass, the brain increases in mass by 0.7544g in monogynous species", message = "The model was fitted to log transformed data so this is not correct"),
  answer("In polygynous species, on average, for an increase of 1 in the log body mass, log brain mass increases by 0.3777", correct = TRUE)
)
)
```

<br>
<details><summary>**Click here for more on the model interpretation**</summary>

Hopefully you've been able to get a good idea of what the model is telling us but if not:

* The model overall can be thought of a fitting two separate lines to the relationship between log male mass and log brain mass, one for monogynous males and one for polygynous males. 

* The interaction term defines the difference in slopes between the two groups defined by th factor levels. Because the interaction term is statistically significant we can interpret this as telling us that there is a significant difference in slopes.

We can visualise the data by plotting the data with the two lines. We'll use the `goodline()` function that was referred to in the video to keep the lines within the data they are fitted to. See if you can fill in the `XXXXX` sections in this code: we're going to use a nicer palette than the default, and add a legend.


```{r eval = FALSE}
goodline <- function(x, a, b, width = 1, colour = "black", type = 1){
  x1 <- min(x)
  x2 <- max(x)
  y1 <- a+b*x1
  y2 <- a+b*x2
  lines(c(x1, x2),
        c(y1, y2),
        lwd = width,
        col = colour,
        lty = type)
}
```

```{r goodline}
goodline <- function(x, a, b, width = 1, colour = "black", type = 1){
  x1 <- min(x)
  x2 <- max(x)
  y1 <- a+b*x1
  y2 <- a+b*x2
  lines(c(x1, x2),
        c(y1, y2),
        lwd = width,
        col = colour,
        lty = type)
}
```

```{r pinniped_plot, exercise = TRUE, exercise.setup = "goodline", exercise.lines = 35}

# Set custom palette
palette1 <- c("chocolate1", "cadetblue4")

# Plot data
plot(
  XXXXX ~ XXXXX,
  data = XXXXX,
  pch = 16,
  col = palette1[Mate_type],
  xlab = "Log Male Mass (Kg)",
  ylab = "XXXXX"
)

# Line for monogynous males
goodline(
  x = log(pinniped$Male_mass_Kg[pinniped$Mate_type == "mono"]),
  a = XXXXX,
  b = 0.7544,
  colour = palette1[1],
  width = 2
)

# Line for polygynous males
goodline(
  x = log(pinniped$Male_mass_Kg[pinniped$Mate_type == "XXXXX"]),
  a = 4.0052,
  b = XXXXX,
  colour = palette1[2],
  width = 2
)

# Add legend
legend("topleft",
       fill = palette1,
       legend = c("Monogynous", "Polygynous"))
```

```{r pinniped_plot-solution}

# Set custom palette
palette1 <- c("chocolate1", "cadetblue4")

# Plot data
plot(
  log(Male_brain_g) ~ log(Male_mass_Kg),
  data = pinniped,
  pch = 16,
  col = palette1[Mate_type],
  xlab = "Log Male Mass (Kg)",
  ylab = "Log Male Brain Mass (g)"
)

# Line for monogynous males
goodline(
  x = log(pinniped$Male_mass_Kg[pinniped$Mate_type == "mono"]),
  a = 2.0779,
  b = 0.7544,
  colour = palette1[1],
  width = 2
)

# Line for polygynous males
goodline(
  x = log(pinniped$Male_mass_Kg[pinniped$Mate_type == "poly"]),
  a = 4.0052,
  b = 0.3777,
  colour = palette1[2],
  width = 2
)

# Add legend
legend("topleft",
       fill = palette1,
       legend = c("Monogynous", "Polygynous"))
```


</details>

### Final point: independence of data

As we've discussed in earlier tutorials, independence of data is a fundamental assumption behind this sort of analysis and also something that you can't really check: rather you have to think a out it when designing your study. Are the data points in this study independent? No --- because some of the species are more cloesly related to some of the other species, and so these are likely to be more similar than we'd expect by chance even in the absence of any effect of mating system. As an example of this take a look at the species in this dataset with male body masses of less than 100Kg.

```{r}

pinniped[which(pinniped$Male_mass_Kg < 100),]
```

Six of these are from the genus *Phoca*, and all of these have monogynous males. What's more, these are all the species from the genus *Phoca* on the dataset. There's an obvious capacity here for apparent patterns in the dataset to be driven by evolutionary history rather than other factors. It could simply be the case that the common ancestor of the genus *Phoca* was small and monogynous, and that any apparent tendency for smaller males to be monogynous is a consequence of that rather than any evolutionary tendency towards small size in monogynous species. This is not something that we can correct for in this analysis here, but there are techniques for correcting for phylogenetic non-independence such as Phylogenetic Generalised Least Squares or PGLS. Yhis is what Fitzpatrick et al. used in their paper: I'm pleased to say that the fundamental conclusions were unchanged.



## Exercise 2: Insulin-like peptides and beetle weaponry


In many species of animal one sex (usually the male, but not always) carries weapons such as horns, enlarged mandibles or similar which are used in contests with other animals from the same species. These weapons are often extremely variable in size between individuals, and an interesting question is why this should be and how the growth of these weapons is regulated. In 2019 a team of scientists based in Japan and the USA^2^ published the results of an in-depth study on the regulation of weapon size in a beetle called the broad-horned flour beetle, *Gnatocerus cornutus*. Males of this species have enlarged mandibles which they use in fights with other males to gain access to females. There is considerable variation in the length of these mandibles, much of which is known to be associated with the diet of the males during development. 

Insulin-like peptides (ILPs) have been suggested to be important in the regulation of weaponry in insects, and Okada and co-workers identified five ILP genes in the *G. cornutus* genome. To test whether they were involved in weapon growth, they silenced these genes using RNA interference (RNAi) in final instar larvae, and then measured the mandible size and also a number of other measures of size in the adult males. RNAi coding for green flourescent protein (GFP) was used as a control.

Here, we're going to model the relationship between RNAi treatment and mandible length. Because mandible length increases with body size, we'll also include elytron width in our model as a measure of body size. The data set is on github at https://github.com/rjknell/Linear_models/raw/master/data/Okada2019.csv: read this into R as an object called `beetle` and check the import using `str()`.

```{r beetle_import, exercise = TRUE}

```

```{r beetle_import-solution}
beetle <- read.csv("https://github.com/rjknell/Linear_models/raw/master/data/Okada2019.csv")

str(beetle)
```

There are three variables: `treatment` which is the RNAi treatment the beetle received, `EW` which is Elytra Width in µm and `ML` which is Mandible Length, also in µm. `ML` is our response variable and `EW` is a continuous explanatory variable. `treatment` has discrete values and we will be wanting to use this as a factor in our model. Before we do anything else, then, we need to ask R to make it into a factor using the `as.factor()` function. Check the conversion has gone correctly with the `summary()` function.

```{r treatment_factor, exercise = TRUE}

```

```{r treatment_factor-hint-1}
# Remember that you have to make treatment
# into a factor and then allocate it to the
# correct variable in the beetle data frame.
# 
# Then run summary() on beetle$treatment
# 
```

```{r treatment_factor-hint-2}
# This is the solution
beetle$treatment <- as.factor(beetle$treatment)

summary(beetle$treatment)
```

`GFP` refers to the beetles given RNAi for Green Flourescent Protein (a jellyfish protein) as a control, and `1ILP` to `5ILP` are the five insulin-like protein genes which were suppressed using RNAi in the final instar larvae. Let's draw a scatterplot of `ML` against `EW`, colour coded by the levels of `treatment` to get an idea of what our data look like.

Because we have lots of factor levels we'll use a colour palette from the RColorBrewer package. This package gives R users access to the colour palettes from [the color brewer website](https://colorbrewer2.org/). The package will be automatically installed if you are using a recent version of R, otherwise you might need to install it yourself using `install.packages("RColorBrewer")`. We can generate a palette with 6 colours like this:
  
```{r}
library(RColorBrewer)

palette1 <- brewer.pal(n = 6, name = 'Dark2')
```

Now to draw our scatterplot. See if you can fill in the XXXXX parts.

```{r beetle_scatterplot1, exercise = TRUE, exercise.lines = 15}

plot(XXXXX ~ XXXXX,
     data = XXXXX,
     col = palette1[treatment],
     pch = 16,
     xlab = "XXXXX",
     ylab = "XXXXX"
)

legend("topleft", 
       col = XXXXX,
       pch  =XXXXX,
       legend = XXXXX
)
```

```{r beetle_scatterplot1-hint-1}
# The plot should be relatively easy: 
# we want ML plotted against EW with the
# dataframe as beetle. For the axis labels
# don't forget the units are µm 
# 
# For the legend col and pch should
# be the same as for the plot
# 
# The legend text (the legend = argument)
# needs to be the levels of the treatment
# factor 

```

```{r beetle_scatterplot1-hint-2}
# This is the solution
plot(ML ~ EW,
     data = beetle,
     col = palette1[treatment],
     pch = 16,
     xlab = "Elytron width (µm)",
     ylab = "Mandible length (µm)"
)

legend("topleft", 
       col = palette1,
       pch  =16,
       legend = levels(beetle$treatment)
)
```

Have a look at the plot and the patterns in the data. 

```{r beetle_scatterplot1_quiz, echo=FALSE}


question("Based on the scatterplot above, which of the following do you think are correct? More than one question can be correct",
  answer("The relationship between elytron width and mandible length is curved and it will be necessary to use a quadratic term in our model", message  = "Answer 1: It's hard to be absolutely sure about this but there's no clearly curved relationshipds there."),
  answer("The beetles treated with 1ILP seem to have much shorter mandibles than do the others.", correct = TRUE),
  answer("There is one data point with an extreme value for mandible which we should consider possibly removing as an outlier", correct = TRUE),
  answer("Both variables show strong positive skew and a data transformation such as a log transformation will be necessary before further analysis", message = "Answer 2: There is nothing here to indicate positive skew"),
  answer("The two data points for beetles with an elytron width of less than 1200 µm appear to be anomalously small and we should consider removing these as outliers", message = "These were small beetles but there's nothing to indicate that their size is sufficiently small that we should consider them as being very unlikely values or as potentially having been generated by a different process from the ones we're interested in.") 
)
```
<details></summary>**Click here for more on what to do with these data**</summary>

As a rule of thumb, it's always best to avoid removing data from an analysis for any reason. Nonetheless, we do sometimes get data points that just look like they shouldn't be there, and sometimes they really shouldn't: errors can be made during measurement and recording data, and other errors can be made such as misidentification of individuals, mistakes in care of experimental plants, animals or cultures or in the preparation of reagents and so on. Detecting and dealing with such anomalies is fraught with risk because because the potential for bias and unconsciously altering the results to fit a particular hypothesis is so high. In the mind of your author, the only time when it's really justified is when there is obviously something anomalous about a data point which means that you can be sure that it should not be there. Impossible values such as negative values when the variable in question cannot be negative, or fractions when the data can only be integers, are clearly anomalous and an indicator that an error has been made in measuring or recording the value. When it comes data points with values that are just unlikely, however, things are much more difficult. In these cases it's good practice to think about four questions. 

Is the data point clearly so far outside the range that would be expected given the distribution of the rest of the data that in all probability an error has been made?

Based on your knowledge of the system, is the value of the data point one that is possible?

Is it possible that the experimental protocol might have caused this extreme value?

Is this single data point likely to have an disptoprtionate effect on the outcome of the analysis?

In this case, the individual with the mandible length of 921 µm has a mandible length that is very much higher than all of the rest. Furthermore, there's no indication of positive skew in the remainder of the data which could lead to a few very high values. Examining the data more closely also shows us something noteworthy:

```{r}
beetle[15:30,]
```
Have a look and see if you can spot it. The anomalous data point is number 24.

<details><summary>**Click here for more**</summary>

The ML value for data point 24 is 921 and is recorded with no decimal places. All of the other values for ML are recorded to two decimal places. This suggests that there has been an error in recording the value </details>






and what we know about the system that there must have been an error in its recording or some other problem. In these cases, where it's possible to make a clear argument that the data point should not be there 

</details>

<hr>
  2. Okada, Y., Katsuki, M., Okamoto, N., Fujioka, H. & Okada, K. (2019) A specific type of insulin-like peptide regulates the conditional growth of a beetle weapon. PLoS biology, 17, e3000541.
