---
title: "Linear models 4: Multiple Regression"
output: 
  learnr::tutorial:
    progressive: false
    theme: cerulean
    highlight: textmate
    css: css/test.css
    code_folding: hide
runtime: shiny_prerendered
author: Rob Knell
description: >
  Linear models with more than one continuous explanatory variable: how to fit models, check diagnostics and interpret the output.
---

```{r setup, include=FALSE}
library(learnr)
library(gplots)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE, fig.width = 5, fig.height = 5)

finch <-
  read.csv(
    "https://github.com/rjknell/Linear_models/raw/master/data/Hill_et_al_finch_coloration_trimmed.csv"
  )


```

## Multiple continuous explanatory variables.

In much the same way that we can extend the simple ANOVA type linear model to include multiple explanatory factors, we can extend the simple regression type linear model to include multiple continuous explanatory variables --- an analysis that's often called *multiple regression*. This has the same advantages as fitting multiple explanatory factors: we carry out our analysis as a single model fitting exercise rather than as a series of individual analyses; we can detect effects that would otherwise not be found; we can control for the effects of variables statistically; and we can check for interaction terms between our variables. Much of the procedure for dealing with multiple continuous explanatory variables is similar to that for multiple factors so we can jump straight into some exercises.

## Exercise 1: house finch colouration and mitochondrial function

The data we will analyse here come from a study published in 2019 by Geoffrey Hill of Auburn University and co-workers^1^. This research was based on the observation that many animals use visual signals with carotenoids as the pigment in question, and it's now well known that in many cases the redder the visual signal the better the signaller performs in terms of acquiring mates or competing for resources. The mechanism linking red carotenoid signalling to individual quality is not known, however, but Hill *et. al.* followed previous work which had found that many bird species ingest yellow carotenoids but then oxidise these to red pigments, probably in the mitochondria. One part of Hill *et al.*'s investigation into this involved measuring the hue of the red feathers and a series of measures of mitochindrial function in 36 male house finches *Haemorhous  mexicanus* at a time when they were moulting and therefore actively producing red carotenoids.

<img src = "https://github.com/rjknell/Linear_models/raw/master/inst/tutorials/Linear%20models%204%20multiple%20regression/house_finches.jpg" width = "500">

**Figure 1** Male and female house finches showing the sexually dimorphic red colouration of the male. Photo by Donald Willin, released under a [creative commons attribution 2.0 licence](https://creativecommons.org/licenses/by/2.0/).

The dataset which we'll analyse is a subset of a larger dataset which was analysed by Hill *et al.*. Let's load it into R and check its structure.

```{r}
finch <-
  read.csv(
    "https://github.com/rjknell/Linear_models/raw/master/data/Hill_et_al_finch_coloration_trimmed.csv"
  )

str(finch)
```

We have `Hue` as a response variable. Higher values of `Hue` indicate yellower plumage and lower values redder for a particular bird. There are then three measures of mitochondrial function: firstly, `C1RCR` which refers to the "Respiratory Control Ratio", calculated by dividing the maximum respiration rate by the basal rate. Secondly we have `C1MMP` which refers to the Mitochondrial Membrane Potential, and finally there is `PGC_1a`, a measure of the levels of a protein which is activates transcription in mitochondrial biogenesis, so a measure of the rate at which mitochondria are replaced.

As always we'll start with some exploratory graphs. In this case we'll draw scatterplots of Hue against our three measures of mitochondrial function. See if you can fill in the missing variables in this code block

```{r fig.height = 10, scatterplots, exercise = TRUE}

par(mfrow = c(3,1))

plot(Hue ~ C1RCR,
     data = finch,
     pch = 16,
     col = "aquamarine4",
     ylab = "Hue",
     xlab = "Respiratory control ratio")

plot(XXXXX ~ XXXXX,
     data = XXXXX,
     pch = 16,
     col = "aquamarine4",
     ylab = "Hue",
     xlab = "Mitochondrial membrane potential")

plot(XXXXX ~ XXXXX,
     data = XXXXX,
     pch = 16,
     col = "aquamarine4",
     ylab = "Hue",
     xlab = "PGC 1a")

par(mfrow = c(1,1))
```

```{r scatterplots-hint-1}
# What you need to do is to put the relevant
# variable names into the formulas in the 
# second and third plot function calls, and 
# fill in the name of the data frame in the
# data = argument
```

```{r scatterplots-hint-2}
# This is the solution
par(mfrow = c(3,1))

plot(Hue ~ C1RCR,
     data = finch,
     pch = 16,
     col = "aquamarine4",
     ylab = "Hue",
     xlab = "Respiratory control ratio")

plot(Hue ~ C1MMP,
     data = finch,
     pch = 16,
     col = "aquamarine4",
     ylab = "Hue",
     xlab = "Mitochondrial membrane potential")

plot(Hue ~ PGC_1a,
     data = finch,
     pch = 16,
     col = "aquamarine4",
     ylab = "Hue",
     xlab = "PGC 1a")

par(mfrow = c(1,1))
```

Looking at these plots we can see that there might be relationships between some of these variables and the redness of the bird's plumage. Recalling that high values for `Hue` mean yellower feathers, there might be a positive relationship between Hue and PGC 1a expression, and there might be negative relationships with respiratory control ratio and mitochondrial membrane potential (meaning in the last two cases that "better" individuals with redder feathers have higher values for RCR and MMP). More usefully, there's no indication of anything that might cause problems in our analysis: no wildly implausible or potentially problematic data points and everything looks quite well behaved. One data point for MMP does seem a little low given the distribution of the others but not to the extent that we need to worry much about it. Let's fit a model. 

Fill in the formula in the `lm()` function call. We want to eplxain the patterns in `Hue` and we'll fit just the main effects of `C1RCR`, `C1MMP` and `PGC_1a`: we won't test for interaction terms here.


We'll use `drop1()` to check on the significance of each explanatory variable.

```{r model_1, exercise = TRUE}
M1 <- lm(, data = finch)

drop1(M1, test = "F")
```
```{r model_1-hint-1}
# Hue is the response variable so that goes on the left of
# the tilde. The three explanatory variables go on the
# right hand side of the tilde, separated by plus symbols
```

```{r model_1-hint-2}
# This is the solution:
M1 <- lm(Hue ~ C1RCR + C1MMP + PGC_1a, data = finch)

drop1(M1, test = "F")
```

According to our deletion test two of our three explanatory variables have statistically significant effects on the redness of the finch's feathers. The third, mitochondrial membrane potential, does not. Depending on our opinoin of the various arguments surrounding different model selection techniques, we could potentially choose to remove this non-significant term from our model to give us a *minimal adequate model* to describe the patterns in these data (see the tutorial on model selection for more on this). Since `C1MMP` is not a variable which we have introduced into the model to allow us to control for an effect, and it is not a variable that is in the model because it represents some important element of experimental design (e.g. if the design were a blocked one we should include `block` as a factor) we can argue that removing it is justified. 

Fit a second model called `M2` and assess the significance of the explanatory variables using `drop1()`

```{r model_2, exercise = TRUE}

```
```{r model_2-hint-1}
# Just use the code from the previous fit with C1MMP
# removed and M1 replced by M2
# 
# Don't forget to change the name of the model in the
# drop1() function call
```

```{r model_2-hint-2}
# This is the solution

M2 <- lm(Hue ~ C1RCR + PGC_1a, data = finch)

drop1(M2, test = "F")
```

This leaves us with a model with two significant explanatory variables. We should check our diagnostic plots now.

```{r echo = FALSE}
M2 <- lm(Hue ~ C1RCR + PGC_1a, data = finch)
```

```{r}
plot(M2, which = 1:2)
```

Have a think about the diagnostic plots and try to answer the questions.

```{r diagnostics-quiz, echo=FALSE}
  question(
    "Which of the following statements is correct?",
    answer("The residuals versus fitted values plot shows evidence of strong heteroskedasticity", message = "Answer 1. Heteroskedascticity means that the variance is varying across the range of fitted values, often leading to a plot with a characteristic wedge shape. There is no suggestion of that here"),
    answer("It is not clear but there is some indication of negative skew", message = "Answer 2. There's nothing here to indicate negative skew, which would show as more very negative residuals and a convex curve of points on the qq-plot"),
    answer("The qq-plot shows that there is some positive skew in the residuals", correct = TRUE),
    answer(
      "The qq-plot shows that the relationship between at least onf of the explanatory variables and the response variable is non-linear", message = "Answer 4. The qq-plot can only tell you about the distribution of the residuals, not about whether the shape of the relationship between response and epxlanatory variables is linear",
      answer("The presence of some residuals with high positive values which are not mirrored in the negative residuals indicates that there might be some positie skew in the residuals", correct = TRUE)
    )
)
```

<br><br>
<details><summary>**Click here for more on the diagnostics**</summary>


Rule 1 of dealing with real data is that your diagnostic plots are rarely going to give you a clear answer about the distributions of your residuals. This is a prime example of this. Both of these plots show us that there is a small amount of positive skew in our residuals: in the plot of residuals versus fitted values you can see some residuals with rather high positive values, and the qq-plot has the characteristic shape associated with positive skew, with the points following a curve with the smallest and the largest residuals both having more positive values than would be predicted if the residuals were following a normal distribution. What isn't clear is whether this amount of skew is likely to making a serious impact on our fitted model. We can check a histogram of the residuals to see if this gives any further enlightenment.

```{r}
hist(M2$residuals, breaks = 10)
```

These are clearly somewhat positively skewed. Overall the amount of skew in the residuals is not especially severe, but we should probably try to do something about it. We could try a square root transformation of these data but to cut a long story short a square root trnasformation doesn't really rectify the problem so a log transformation is necessary. See if you can change this code to log transform the response variable so we can see what this does to the model with all three explanatory variables. There are no zeros in the `Hue` variable so there's no need to add a constant.

```{r log_model1, exercise = TRUE}
L1 <- lm(Hue ~ C1RCR + C1MMP + PGC_1a, data = finch)

drop1(L1, test = "F")
```
```{r log_model1-hint-1}
# Just use the log() function on the Hue variable
```

```{r log_model1-hint-2}
# This is the solution
L1 <- lm(log(Hue) ~ C1RCR + C1MMP + PGC_1a, data = finch)

print(drop1(L1, test = "F"))
```

`C1MMP` remains non-significant but `PGC_1a` is now slightly the wrong side of 0.05. Let's simplify the model by removing the variable with the least support,`C1MMP`, and see what that looks like.

```{r}
L2 <- lm(log(Hue) ~ C1RCR  + PGC_1a, data = finch)

drop1(L2, test = "F")
```
OK, since we removed the less well supported term from the model `PGC_1a` is statistically significant in this model, although not especially so. Let's re-check the diagnostics to see if the log transformation has sorted out the error distribution.

```{r}
plot(L2, which = 1:2)
```
Things are better. The residuals versus fitted values plot no longer has any indication of skew, but there is a hint of a curve from the non-parametric smoother that R kindly draws across it in red. Is that indicative of a serious issue? Probably not in this case since the appearance of a curve is really being caused by a few negative residuals at the highest and lowest fitted values. If you were concerned about this you could try fitting a model with a quadratic term for one of the explanatory variables but we'll leave it there. The qq-plot shows that the log transform has fixed the issues with the most positive residuals although there is still some deviation from the line from the most negative residuals.

</details>
<br><br><hr>

1. Hill, G.E., Hood, W.R., Ge, Z., Grinter, R., Greening, C., Johnson, J.D., Park, N.R., Taylor, H.A., Andreasen, V.A., Powers, M.J., Justyn, N.M., Parry, H.A., Kavazis, A.N. & Zhang, Y. (2019) Plumage redness signals mitochondrial function in the house finch. Proceedings of the Royal Sociaty B: Biological sciences, 286, 20191354.

## Interpreting the fitted model

Take a look at the `summary()` output for the L2 model.

```{r prepare-L2}
L2 <- lm(log(Hue) ~ C1RCR  + PGC_1a, data = finch)
```

```{r summary_log_model2, exercise = TRUE, exercise.setup = "prepare-L2"}


```

```{r summary_log_model2-hint-1}
# This is the solution

summary(L2)
```

Much as the coefficients table for a simple linear regression gives us values we can use to generate the equation for a straight line, the coefficients here can give us a more complex equation relating our explanatory variables to the response variable. For a model with two continuous explanatory variables, the equation for the fitted model is:

$$y = a + b \times x_1 + c \times x_2$$
where $a$ is the intercept, $b$ is the slope for variable $x_1$ and $c$ is the slope for variable $x_2$. This equation allows us to generate predicted values from our model. You will often see linear model equations written with a somewhat different notation:

$$y_i = \beta_0 + \beta_1  x_{1i} + \beta_2 x_{2i} + \epsilon_i, \quad i = 1...n$$
This is the formal notation for a linear model with two explanatory variables. It looks rather more complicated but the fundamentals are the same: $\beta_0$ is the intercept and $\beta_1$ and $\beta_2$ are the slopes for the two variables $x_1$ and $x_2$. The subscript $i$ you can see refers to the "i-th" value in the data set. As an example, if we were thinking about the the third observation then $i$ would be three. $y_i$ in this case would refer to the value of the third observation in the data set, $x_{1i}$ means the third value of $x_1$ and $x_{2i}$ the third value of $x_2$. Finally, $\epsilon_i$ is the *error* term: so each of the $i$ values of $y$ in the dataset will deviate from the predicted value by some degree, and this part of the equation represents that deviation. For a standard linear model this error is assumed to be drawn from a normal distribution with a mean of zero and a fixed standard deviation, hence the assumptions that the errors in a dataset should be normal and the variance should be equal across the dataset.

For the moment we'll stick with the simpler version which will generate the predicted values. We can extract the coefficients from our table above and generate an equation to relate `Hue` to `C1RCR` and `PGC_1a`:

$$log (Hue) = 3.03 - 0.262 \times C1RCR + 7.10 \times PGC\_1a.$$

Whereas simple linear regression gives us a predicted straight line, in this case the equation describes a flat surface, and more complicated models with more than two explanatory variables describe more complex surfaces. These are not easy to visualise: as a general rule even "3D" graphs are poor ways to display data and plotting graphs in higher dimensions tends to be problematic. One option you'll often see is to plot the response variable against each explanatory variable separately and then to show a fitted line from a simple linear regression as an illustration of the way the two are related. We'll do this for our example: I've given you the code for the first plot, see if you can generate the code for the second.

```{r plot_variables1, exercise = TRUE, exercise.lines = 30, fig.width = 6, fig.height = 4}

# Two plots side by side
par(mfrow = c(1,2))

# plot log hue against C1RCR
plot(log(Hue) ~ C1RCR,
     data = finch,
     pch = 16,
     col = "steelblue",
     ylab = "Log of Hue",
     xlab = "C1RCR level")

# use abline() to draw the line from a
# simple linear regression
abline(lm(log(Hue) ~ C1RCR, data = finch),
       col = "blue",
       lwd = 2)

# plot log hue against PGC_1a






# use abline() to draw the line from a
# simple linear regression

# reset plot window to a single plot
par(mfrow = c(1,1))
```

```{r plot_variables1-hint-1}
# You can use the code for the first plot
# and the abline function
# but you need to change the x-variable 
# name and also the x-axis label
```

```{r plot_variables1-hint-2}
# You can use the code for the first plot
# and the abline function
# but you need to change the x-variable 
# name and also the x-axis label
# 
# Make sure you have commas between each argument
# and that you have matching brackets
```

```{r plot_variables1-hint-3}
# This is the solution

# Two plots side by side
par(mfrow = c(1,2))

# plot log hue against C1RCR
plot(log(Hue) ~ C1RCR,
     data = finch,
     pch = 16,
     col = "steelblue",
     ylab = "Log of Hue",
     xlab = "C1RCR level")

# use abline() to draw the line from a
# simple linear regression
abline(lm(log(Hue) ~ C1RCR, data = finch),
       col = "blue",
       lwd = 2)

# plot log hue against PGC_1a

plot(log(Hue) ~ PGC_1a,
     data = finch,
     pch = 16,
     col = "steelblue",
     ylab = "Log of Hue",
     xlab = "PGC 1a level")

# use abline() to draw the line from a
# simple linear regression

abline(lm(log(Hue) ~ PGC_1a, data = finch),
       col = "blue",
       lwd = 2)

# reset plot window to a single plot
par(mfrow = c(1,1))
```


This is certainly illustrative and in this case it gives a reasonable impression of the way these two variables relate to feather hue, but it's not a great representation of the actual fitted model. Because variables in a linear model are fitted to the data with effects of other variables *partialled out*, you can find that including your variables in linear models can reveal effects which are substantially different from those that you would see if you analysed your variables separately, so be careful if you present your data like this.

If you want to show the fitted model and the data more accurately, you can plot the data with the effect of the first variable removed. Our equation for our fitted model in this case is

$$log (Hue) = 3.03 - 0.262 \times C1RCR + 7.10 \times PGC\_1a.$$

So we can generate a new variable which is

$$log(Hue) - (3.03 - 0.262 \times C1RCR)$$

and plot the effect of PGC 1a against that. Here's a code framework: see if you can fill in the missing parts to generate a new plot showing the effect of PGC 1a. To draw in the effect of PGC 1a you need a line with an intercept of zero and a slope equal to the coefficient for PGC 1a in the model.

```{r plot_variables2, exercise = TRUE, exercise.lines = 15}

# Generate new variable with the effects of C1RCR removed

newvar <- XXXXX

# plot the new variable against PGC 1a

plot(XXXX ~ XXXX,
     data = finch,
     pch = 16,
     col = "steelblue",
     ylab = "Log of Hue minus the effect of C1RCR",
     xlab = "XXXX")

# use abline() to draw the line from a
# simple linear regression
abline(a = X, b = X,
       col = "blue",
       lwd = 2)
```

```{r plot_variables2-hint-1}
# To generate newvar:

newvar <- log(finch$Hue) - (3.03 - 0.262 * finch$C1RCR)
```

```{r plot_variables2-hint-2}
# For the plot

plot(newvar ~ PGC_1a,
     data = finch,
     pch = 16,
     col = "steelblue",
     ylab = "Log of Hue minus the effect of C1RCR",
     xlab = "PGC 1a")
```

```{r plot_variables2-hint-3}
# For the line

abline(a = 0, b = 7.1,
       col = "blue",
       lwd = 2)
```

Finally, we need to interpret these results in terms of the biology of the system. Bearing in mind that low scores of Hue indicate male birds with redder plumage, and redness seems to be acting as an indicator of 'condition' in these males, what do these results tell us? For the full story you're best off reading the paper, but briefly:

* Overall there does seem to be a link between mitochondrial functioning and feather colour. The data we have here are correlational, of course, so we have to be cautious in our interpretation because we don't know much about causality.
* More yellow plumage is associated with high levels of PGC 1a. This protein is associated with mitochondrial biogenesis and this result suggests that perhaps birds with redder feathers have lower rates of mitochondrial turnover.
* High RCR values are associated with redder males. RCR is the ratio of maximum respiratory rate to resting rate, and further analysis by the paper's authors showed that the increased RCR in redder males was a consequence of lower resting rate rather than higher maximum rate, suggesting that "better" birds with redder feathers are paying less of a cost for supporting basal respiration.

## Exercise 2: interactions and curves

In 2017 Tom Houslay and co-workers^1^ published a study of the relationship between, among other things, resource availability (food quality) and sexual signalling (calling song) in the decorated cricket, *Gryllodes sigillatus*. Male crickets "sing" to attract females by rubbing specialised areas of their wings together, an activity which is energetically costly. As part of their study, Houslay *et al.* fed freshly eclosed^2^ adult male crickets on synthetic food with a 1:8 protein to carbohydrate ratio but which caried in total nutritional content from 12% to 84%, with the amount of nutritional content increasing in 12% increments between these limits. The crickets were weighed at the start of the experiment and again after a week, and the amount of time they spent singing during the week was recorded.

The 



1. Houslay, T.M., Houslay, K.F., Rapkin, J., Hunt, J. & Bussière, L.F. (2017) Mating opportunities and energetic constraints drive variation in age‐dependent sexual signalling (ed C Miller). Functional ecology, 31, 728–741.

2. Eclosion refers to the moult that an insect undergoes when it becomes an adult.