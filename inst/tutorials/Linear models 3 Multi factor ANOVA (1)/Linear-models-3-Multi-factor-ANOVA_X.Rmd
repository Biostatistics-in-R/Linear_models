---
title: "Linear models 3: Multi-factor ANOVA"
output: 
  learnr::tutorial:
    progressive: false
    theme: cerulean
    highlight: textmate
    css: css/test.css
    code_folding: hide
runtime: shiny_prerendered
author: Rob Knell
description: >
  Analysis of variance with multiple explanatory factors: how to fit models, check diagnostics and interpret the output..
---

```{r setup, include=FALSE}
library(learnr)
library(gplots)
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE, fig.width = 5, fig.height = 5)
mouse_movement <-
  read.csv(
    "https://github.com/rjknell/Linear_models/raw/master/data/McCarthy_mice_activity.csv",
    stringsAsFactors = TRUE
  )

bacteria <-
  read.csv(
    "https://github.com/rjknell/Linear_models/raw/master/data/Robinson_zebra_fish_bacteria.csv",
  )
```

## Adding extra variables to a linear model

So far we've seen how the linear model can cope with both factors and continuous variables as explanatory variables, and that the way that variance is partitioned for both types of epxlanatory variable is fundamentally the same. Linear models can also cope with more than one explanatory variable, so you can analyse the effect of several different things that might influence your variable of interest at the same time. This has some important benefits over analysing each explanatory variable's effect separately:

* Fitting a single model reduces the number of statistical tests which we need to do, reducing the probability of type 1 errors.

* We can sometimes detect effects which we would not see in single variable analyses, because the linear model allows us to ask the question "what is the effect of variable Y when we have already taken the effect of variable X into account?".

* We can statistically control for variables if we wish to: so in a study with height and weight as explanatory variables we can ask the question "What is the effect of weight when height has been taken into account?" --- in other words, what is the effect of an individual being heavier or lighter than would be expected for someone of that height.

* We can look for *interactions* between explanatory variables which arise when the effect of one variable depends on the level of the other.

This video explains how the variance is partitioned when there is more than one explanatory factor and what some of the consequences of this are.

VIDEO

In this tutorial we'll look at analysing linear models with more than one factor as the explanatory variables. These linear models are often called "multi-factor ANOVA" or you might see them called a "two-way ANOVA" or similar depending on the number of explanatory factors used.

## Two-factor linear model example

The data we'll be looking at here come from a study on the effect that paternal exposure to nicotine has on the behaviour of their descendants, as published by McCarthy and co-authors in 2018^1^. The dataset we will use is a small subset of what was presented in their paper, and deals with  the spontaneous locomotor activity that the F1 offspring of nicotine exposed or control males mated with unexposed females exhibited over a 12 hour period. Locomotor activity was measured by placing the animals in a testing chamber with a series of infra-red beams arranged in a grid, and each time a beam was broken this was logged as a single locomotory event. Our dataset includes data from both male and female F1 offspring.

Let's load it from github and check its structure.

```{r}

mouse_movement <-
  read.csv(
    "https://github.com/rjknell/Linear_models/raw/master/data/McCarthy_mice_activity.csv",
    stringsAsFactors = TRUE
  )

str(mouse_movement)
```
Because we specified `stringsAsFactors = TRUE` as an argument both `Treatment` and `Sex`have been imported as factors. `SLA` (Spontaneous Locomotory Activity) is our response variable and is the count of all the recorded times an infra-red beam was broken during the 12-hour period. `Sex` is the sex of the F1 mouse in question, and `Treatment` is the treatment the father of the mouse was exposed to: "water" where the mouse was given plain drinking water and "nicotine" where the mouse was given drinking water containing 200Âµg/mL nicotine for 12 weeks. Let's look at our sample sizes for each factor combination.

```{r}
table(mouse_movement$Sex:mouse_movement$Treatment)
```

The data are somewhat unbalanced, with sample sizes ranging from 11 to 18 and overall rather more data for the controls whose fathers were given water but there's no combination with a really small or large sample size by comparison to the others. Now let's vsualise these data with a boxplot.

```{r fig.cap = "**Figure 1.** Boxplot showing Spontaneous Locomotor Activity (SLA) as measured in IR beam triggers over 12 hours for mice with paternal expsoure to either nicotine or water."}

boxplot(SLA ~  Treatment * Sex, 
        data = mouse_movement,
        xlab = "Treatment & Sex",
        ylab = "SLA(movements/12h)")
```

Looking at this we can see some patterns: spontaous movement rates seem more common in females and in animals with fathers exposed to nicotine, but as always we need to do some statistics to give us an idea of how confident we can be that thses patterns aren't just a consequence of sampling error. The boxplots are also at least approximately symmetrical which tells us that these data aren't strongly skewed. Ther do seem to be some differences in variance between the groups such that the mice with fathers exposed to nicotine have rather wider IQRs than those whose fathers were given plain water. It's not 100% clear how severe this is but we should keep an eye out for evidence of heterogeneous variances when we look at our diagnostic plots.

Fitting linear models with multiple explanatory variables in R simply involves adding new elements to the formula that is the first argument of the `lm()` function.

`SLA ~ Treatment` will fit a model with `Treatment` as the explanatory factor.

`SLA ~ Treatment + Sex` will fit a model with both `Treatment` and `Sex` as the explanatory factors but with no interaction between them --- in other words the *main effects* of the two factors only.

`SLA ~ Treatment + Sex + Treatment:Sex` will fit a model with both `Treatment` and `Sex` as explanatory factors but also with the interaction between the two (specified here by `Treatment:Sex`).

`SLA ~ Treatment * Sex` will, in this case, fit the same model as the previous example. The asterisk `*` means "fit all the main effects and also all the interactions. Here there can only be one interaction because there are only two explanatory factors. If there were a third factor, however, for example `Diet` (good or bad) then

`SLA ~ Treatment*Sex*Diet` would fit all three main effects, the two-way interactions between `Treatment` and `Sex`, between `Treatment` and `Diet` and between `Diet` and `Sex`, and also the three-way interaction between `Treatment`, `Sex` and `Diet`.

`SLA ~ Treatment + Sex + Diet + Treatment:Sex + Treatment:Diet` will fit all three main effects plus two interaction terms only, namely those between `Treatment` and `Sex` and between `Treatment` and our fictional `Diet` factor.

Let's fit a model with both main effects and the interaction, and then bring up an ANOVA table to get an idea of the statistical significance of our explanatory variables.

```{r eval = FALSE}
mouse1 <- lm(SLA ~ Treatment * Sex, data = mouse_movement)

anova(mouse1)
```

```{r echo = FALSE}
mouse1 <- lm(SLA ~ Treatment * Sex, data = mouse_movement)

print(anova(mouse1))
```

The basic structure here is familiar from the single factor ANOVA tutorial. Instead of simply partitioning the variance into that explained by the treatment and the remaining, error variance, however, we have now partitioned it into that explained by the two main effects of `Treatment` and `Sex`, plus a further sum of squares etc. for the interaction between the two and then the error variance (which R calls the `Residuals` of course). The various elements such as the Mean square values and the F-statistics are calculated in the same way as for any other linear model, so MS Sex  = SS Sex (253190901) divided by df Sex (1), and F for Sex is MS Sex/MS Residuals (or error) = 253190901/19714072 = 12.84316 on 1 and 50 df which is highly significant (p = 0.000767).

1: McCarthy, D.M., Morgan, T.J., Jr, Lowe, S.E., Williamson, M.J., Spencer, T.J., Biederman, J. & Bhide, P.G. (2018) Nicotine exposure of male mice produces behavioral impairment in multiple generations of descendants. PLoS biology, 16, e2006497.

## Model diagnostics

Before we go any further with interpreting this model, however, we should check our diagnostic plots to make sure that the assumptions that we make about our data are met, or at least are approximately met.

```{r}
plot(mouse1, which = 1:2)
```

The residuals versus fitted values plot shows little to concern us. The differences in variance between groups that we saw in the boxplot earlier aare there but overall these are not of the degree of magnitude that might cause us serious concern. The qq plot is a bit odd with some deviation from the line of equality but most of the deviations aren't too major. There are some datapoints that have high residual values (6, 48, 38) but again these are not enough to really cause a lot of concern. It's a bit of a judgement call in this case but it's most likely OK. We can do a further check by drawing a histogram of the residuals.

```{r}
hist(mouse1$residuals)
```

You can see from the histogram that the distribution of the residuals is a bit pointier than a normal distribution, with a big sharp peak in the centre of the distribution, and there's a suggestion of some positive skew. Overall though the distribution of the residuals is at least approximately normal and close to symmetrical. Given the general robustness of ANOVA to small to medium deviations from the fundamental assumptions about the data these fairly small departures from the ideal distribution are not likely to be affecting our output in any substantial way.

As a side note, this sort of error distribution is one of the main reasons why these tutorials are all using real data from real science. When you fit models to real data, there is *always* something about the diagnostic plots that makes you scratch your head. You never see residuals perfectly lined up in your qq plot, or with no pattern whatsoever in the residuals versus fitted values. The thing to ask yourself is whether whatever weirdness you're seeing is of a magnitude that might change the results of your analysis. If it is (and knowing what will and what won't is often a matter of experience) then you need to do something about it. If it isn't then don't. If you're not sure, then maybe try something (data transformation, removing highly influential and worrying data points), refit your model, compare it with the original and see if it makes a difference. If it doesn't then you know you're good.


## Significance of model terms

Now that we're reasonably happy about our diagnostics, let's look at the output of our analysis again. We've already seen that we have a non-significant interaction term but the two main effects are significant. How reliable are these p-values though? Take a look at what happens when we change the order of the explanatory variables in our model formula. Here's that first ANOVA table again.

```{r eval = FALSE}
# ANOVA table for first model
anova(mouse1)
```

```{r echo = FALSE}
print(anova(mouse1))
```

Here's the ANOVA table for the model with the explanatory variables in the opposite order.

```{r eval = FALSE}
# Fit model with Sex first instead of treatment
mouse2 <- lm(SLA ~ Sex *Treatment, data = mouse_movement)

# ANOVA table for second model
anova(mouse2)
```

```{r echo = FALSE}
# Fit model with Sex first instead of treatment
mouse2 <- lm(SLA ~ Sex *Treatment, data = mouse_movement)

# ANOVA table for second model
print(anova(mouse2))
```

The statistics for the two main effects change depending on whether their order in the formula. When treatment is first we have F~1,50~ = 15.44, p = 0.000262, but when treatment is the second factor in the model we have F~1,50~ = 13.79, p = 0.000516. This is a consequence of the way that a linear model is fitted: as explained in the video at the start of the tutorial the first term is based on the sum of squares etc. calculated from the raw data, but for the second term the sum of squares etc. is calculated using the residual sum of squares once the sum of squares for the first term has been removed. This method of calculating the sums of squares is called *type I sums of squares* or sometimes *sequential sums of squares*, and is why the SS, MS, F-statistic and p-value can change depending on the order of the terms in the model --- this will happen so long as there is any kind of correlation between the explanatory variables, and even when the explanatory variables are factors this will happen unless the design is perfectly balanced, with equal sample sizes for every combination of treatments.

Do we need to worry about this here? Maybe. One option is to use something called the *type III sums of squares* --- a different way of calculating the sums of squares which treats every term as if it were the last term entered into a model, so in this case the SS for Sex is calculated based on data which has already had the SS for Treatment and for the interaction partitioned out, and the SS for Treatment is calculated based on data which has already had the SS for Sex and for the interaction partitioned out. Some statistical software (e.g. SAS) uses these type III sums of squares by default, so if your analysis is giving a different result from one done with different software then this is worth bearing in mind as a possible explanation. There are also type II sums of squares but these only work in models where there is no interaction term.

To calculate the type III sums of squares we can use the `Anova()` function from the `car` package. Note that this function has a capital A to distinguish it clearly and straightforwardly from the `anova()` function we've already used. What could possibly go wrong?

```{r eval = FALSE}
library(car)

Anova(mouse1, type = "III")
```
```{r echo = FALSE}

library(car)

print(Anova(mouse1, type = "III"))
```

This supports what we've already calculated using the type I sums which are the default option in R: both main effects are significant but the interaction term is non-significant.

Since the interaction is very far from significant, and there's nothing to indicate that there is an important interaction effect here, we can consider generating a new and more parsimonious model without the interaction term. Ther is more in the tutorial on model selection on the reasoning behind this and when it might or might not be justified.

```{r eval = FALSE}
mouse3 <- lm(SLA ~ Treatment + Sex, data = mouse_movement)

Anova(mouse3, type = "III")
```

```{r echo = FALSE}
mouse3 <- lm(SLA ~ Treatment + Sex, data = mouse_movement)

print(Anova(mouse3, type = "III"))
```

Our main effects remain statistically significant in the new reduced model. Now that we have a fitted model and we're happy with our understanding of the signficance of the various terms, let's think about interpreting it.

## Interpreting the fitted model

Let's look at the `summary()` output for our model. You might want to have a look at the video on interpreting this output for ANOVA type models.

VIDEO

```{r}
summary(mouse3)
```

We have an ANOVA with two factors, each of which has two treatments. This gives us a fairly straightforward coefficients table. As with the single factor ANOVA models, the first row of the coefficients table is labelled `(Intercept)`. Instead of telling us the estimated mean for the treatment that comes first in the alphabet this is now the estimated mean for the treatment combination with the levels for each factor that are first alphabetically. Our factor levels are

```{r}
levels(mouse_movement$Treatment)
levels(mouse_movement$Sex)
```

So the intercept for this model is the estimated mean for female mice with fathers treated with nicotine. We can check this by calculating the mean directly.

```{r}
with(mouse_movement,
     mean(SLA[Sex == "Female" & Treatment == "Nicotine"]))
```

The estimate from the model is very close to the actual mean but not dead on. This is because we have no interaction in the model and consequently it is not estimating every single mean exactly: rather the intercept is estimated and then the effects of `Treatment` and `Sex`.

As with single factor ANOVA, we have a standard error, t-statistic and p-value for the intercept which tells us nothing more than this mean is significantly different from zero.

The next row is labelled `TreatmentWater`.

```
               Estimate Std. Error t value Pr(>|t|)  
TreatmentWater    -4545       1212  -3.750 0.000453 ***
```

 This coefficient gives the estimated difference between the mean for mice with paternal nicotine treatment (the intercept) and mice with paternal water treatment - mice whose fathers were treated with water cause 4545 fewer triggers per 12 hours. Because we have no interaction term in our model this applies to both sexes of mice equally: looking at this from the perspective of the nicotine treatment, both male and female mice whose fathers were treated with nicotine cause, on average, 4545 extra triggers per night. In other words, paternal nicotine treatment is associated with a considerable increase in spontaneous locomotor activity --- almost a third more in fact. The standard error, t-test and the small marginal p-value for this row of the coefficients table tells us that an effect this big, or bigger, is unlikely to have arisen simply by sampling error.

The last line is labelled `SexMale`.

```
               Estimate Std. Error t value Pr(>|t|)  
SexMale           -4365       1206  -3.619 0.000678 ***
```

Much as the previous line gives us the effect of paternal exposure, this coefficient gives us the effect of sex. The intercept was for females, and since the estimate for males is -4365 this tells us that on average males will cause 4365 fewer triggers per 12 hours, whether or not their fathers were exposed to nicotine.

Because we have no signficant interaction between our explanatory factors, their effects are not dependent on the value of the other explanatory factor. In other words, the effects of each factor are *additive* in that to get the model prediction for a particular combination of factor levels we can simply add the relevant effects together. So for a male with paternal exposure to nicotine the predicted value is 

Intercept (14554) + the coefficient for Sex (-4365) = 10189, 

whereas for a male with paternal expsore to water the predicted value is

Intercept (14554) + the coefficient for Sex (-4365) + the coefficient for Treatment (-4545) = 5644.

Do we need to go any further with interpreting this model, for example by doing a post-hoc test to assess exactly which means are different from which other means? No. We know the effects of both `Sex` and `Treatment` and we can express those in terms of the actual behaviour of these mice. We know that these main effects are statistically highly significant, and we know that the interaction term is far from significant and can safely be ignored. Further post-hoc testing would not really be helpful.

Let's finish by plotting out our means and confidence intervals. This is a fairly complicated piece of code because we're also plotting the data over the top, and drawing the x-axis labels in using the `axis()` function twice, once for each line.

```{r fig.cap = "**Figure 2.** Spontaneous Locomotor Activity (SLA) as measured in IR beam triggers over 12 hours for mice with paternal expsoure to either nicotine or water. Diamonds indicate means and error bars are 95% confidence intervals."}
# Load the gplots package so we can use 
# the plotmeans() function 
library(gplots)

# Plot means and 95% CIs. Note no x-axis (xaxt = "n") or x-axis label  
# (xlab = "") plotted, no lines connecting the means (connect = FALSE), 
# no sample size indicators (n.label = FALSE)
# and we are drawing the means and CIs larger and thicker
# than the defaults (cex = 2.5, barwidth = 2). 
plotmeans(
  SLA ~ interaction(Treatment:Sex),
  data = mouse_movement,
  n.label = FALSE,
  pch = 18,
  cex = 2.5,
  col = "darkblue",
  barcol = "darkblue",
  barwidth = 2,
  connect = FALSE,
  xlab = "",
  ylab = "SLA (triggers per 12 hours)",
  xaxt = "n",
  ylim = c(0, 24000)
)

# Use points function to draw in the data.
# The extra 88 at the end of the hex code for
# the colour makes the points semitransparent

# This horror show of nested parentheses:
# jitter(as.numeric(interaction(Treatment:Sex)), 0.6),
# uses the interaction() function to generate a factor
# with all the combinations of levels from the interaction
# of sex and treatment, then converts it to numerical
# values, then adds some noise with the jitter() function
# in order to reduce the amount of overplotting for the 
# individual data points in the final graph



points(
  SLA ~ jitter(as.numeric(interaction(Treatment:Sex)), 0.6),
  data = mouse_movement,
  pch = 16,
  col = "#00777788",
  cex = 1.2
)


# Draw in the axis, the axis ticks and the top line of
# the tick labels
axis(
  side = 1,
  at = 1:4,
  labels = c("Nicotine", "Water", "Nicotine", "Water")
)

# Use axis to draw in the "Female" and "Male" labels
# at the appropriate place. Line = 1.5 moves the axis down
# and lwd = 0 means the axis line is not drawn.
axis(
  side = 1,
  at = c(1.5, 3.5),
  line = 1.5,
  labels = c("Female", "Male"),
  lwd = 0
)
```


## Exercise: multi-factor ANOVA 

Here, we will use some data from a study on bacterial adaptation to host gut environments originally published in 2018 by a group of researchers at The University of Eugene, Oregon and the Canadian Institute for Advanced Research^2^. They looked, among other things, at the competitive ability a bacterium called *Aeromonas veronii* after either 4 or 18 passages through the gut of an otherwise germ-free larval zebrafish. Competitive ability was measured *in vivo* by incoulating a larval zebrafish with a dose of both the experimental line of bacteria and an ancestral ine which was tagged with Green Flourescent Protein for 3 days, following which the gut contents were homogenised and the homogenate plated onto tryptone soy agar. After a suitable incubation period the colonies of each strain were counted using a flourescence microscope to distinguish them and a "competitive index" calculated as the ratio of the number of colonies for the adapted line to the colonies from the ancestral line.

The particular experiment which we will look at here is one in which the experimenters compared the competitive ability of bacteria in the host strain which they had been evolving (WT hosts) with the competitive ability of the same bacteria but competing in hosts of a different genetic background (an immunodeficient *myd88~-~* mutant). They also compared the competitive abilities of bacteria which had had a short period of adaptation (4 passages) with bacteria which had a longer period (18 passages). The data are on github at this URL:

https://github.com/rjknell/Linear_models/raw/master/data/Robinson_zebra_fish_bacteria.csv

Start by loading the data into R (I suggest you save the imported dataset as `bacteria`) and checking the dataset structure using `str`


```{r load_bacteria, exercise = TRUE}


```

```{r load_bacteria-hint-1}
# Here's a code framework to help you

object_name <- read.csv("file_location")

str(object_name)
```


```{r load_bacteria-hint-2}
# This is the solution

bacteria <- read.csv(
  "https://github.com/rjknell/Linear_models/raw/master/data/Robinson_zebra_fish_bacteria.csv")

str(bacteria)
```


The response variable is `CI` (Competitive Index) and we have three other variables: `Line`, `Host` and `Passage`. `Line` is a number from 1 to 3 and refers to the fact that the experimenters actually repreated the experiment three times with three independently evolved lines of bacteria. For the present we'll not worry about `Line` but we will include it in a later analysis.

None of our explanatory vectors are currently factors but we need both `Host` and `Passage` to be specified as such. You can do this using the `as.factor()` function. I've done one, you'll need to do the other. Once you've done that check the data frame again using `str()`.

```{r make_factors, exercise = TRUE}
bacteria$Host <- as.factor(bacteria$Host)

```

```{r make_factors-hint-1}
# This is the solution

bacteria$Host <- as.factor(bacteria$Host)
bacteria$Passage <- as.factor(bacteria$Passage)

str(bacteria)
```

All looks good so far. There are two factors to include in our analysis for the moment. `Host` has two treatments, `WT` and `myd88` and `Passage` also has two treatments, `4` and `18`. Before we jump in and fit a model, though, we should have a look at our data. See if you can draw a boxplot of `CI` as explained by `Host` and `Passage`.

```{r bacteria_boxplot, exercise = TRUE}

```

```{r bacteria_boxplot-hint-1}
# You can use the boxplot() function and put in
# the variable names in a formula as you would 
# for a linear model. You need to specify the dataframe
# using the data = argument
```

```{r bacteria_boxplot-hint-2}
# Don't forget to put some sensible axis
# labels in.
# Make sure there are commas between each argument
```

```{r bacteria_boxplot-hint-3}
# Here's a code framework which might help

boxplot(response ~ explanatory1 * explanatory2,
        data = dataframe,
        ylab = "",
        xlab = "")
```

```{r bacteria_boxplot-hint-4}
# Here's a solution

boxplot(CI ~ Passage * Host,
        data = bacteria,
        ylab = "Competitive index",
        xlab = "Treatment combination")
```

What do you see?

```{r boxplot-quiz, echo=FALSE}
  question(
    "What can we say about the shape of the data distributions?",
    answer("There is extreme positive skew", correct = TRUE),
    answer("It is not clear but there is some indication of negative skew"),
    answer("It's not possible to say because the sample size is too small"),
    answer(
      "The boxplots are roughly symmetrical indicating no major problems with the data distribution",
    )
)
```

<details><summary>**Click here for more**</summary>

As you have hopefully seen, these data look like they are very positively skewed. Almost all of the data have values between zero and about 1000, but there are also some datapoints with much higher values. Given that the linear model assumes normality in the errors, or the residuals, rather than in the raw data we should be cautious about worrying about apparent violations in the raw data. In this case however, it's clear that if we were to fit a model to these data our residuals would be exceedingly skewed and also we can imagine that those extreme values would have some disproportionate effects on our fitted model. In this case we can use a data transformation to try to fix this extreme skew, with the two easy options being a square root transformation and a log transformation. Just checking:

```{r}
summary(bacteria$CI)
```

There aren't any zeros in the data so we can use a log transformation without having to add a constant to the dataset. Let's compare what we see with the two different transformations: see if you can modify this code to plot one boxplot of square root transformed data and then another one with log transformed data.

```{r transformations, exercise = TRUE}

boxplot(CI ~ Passage * Host,
        data = bacteria,
        ylab = "Competitive index",
        xlab = "Treatment combination")

boxplot(CI ~ Passage * Host,
        data = bacteria,
        ylab = "Competitive index",
        xlab = "Treatment combination")
```

```{r transformations-hint-1}
# The function for a square root transformation is sqrt()
# The function for a log transformation is log()
```

```{r transformations-hint-2}
# This is the solution
boxplot(sqrt(CI) ~ Passage * Host,
        data = bacteria,
        ylab = "Square root of competitive index",
        xlab = "Treatment combination")

boxplot(log(CI) ~ Passage * Host,
        data = bacteria,
        ylab = "Log competitive index",
        xlab = "Treatment combination")
```

This is a nice illustration of the effect of the two different transformations. You can see that the square root transformation is not sufficient to get rid of the skew in these data, but the log transformation gives us some nicely symmetrical boxplots. We'll do our analysis on log transformed data.
</details>
<br><br><hr>

2. Robinson, C.D., Klein, H.S., Murphy, K.D., Parthasarathy, R., Guillemin, K. & Bohannan, B.J.M. (2018) Experimental bacterial adaptation to the zebrafish gut reveals a primary role for immigration. PLoS biology, 16, e2006893.

## Fitting the model and diagnostics

To start with we will ignore the `Line` variable so we have two explanatory factors and we'd like to fit both of these plus their interaction. Save the fitted model object as `M1`, and then check the significance of the interaction term by bringing up an ANOVA table using `anova()`. We won't worry about type I versus type III sums of squares for the moment since we're really only concerned with the significance of the interaction term. Don't forget to log the response variable and tell `lm()` that you're using data from `bacteria`. As a consequence of some weirdness that happens with the learnr package, please enclose your `anova()` function call in a `print()` function to keep the ANOVA table formatting consistent. Sorry about this.

```{r model_fit_1, exercise = TRUE}


```

```{r model_fit_1-hint-1}
# Use lm() to fit the model with a formula which
# is the same as you used for the boxplots, and 
# remember to specify what the dataframe is with data =

# for the print() stuff it's just
print(anova(M1))
```

```{r model_fit_1-hint-2}
# Here is the solution
M1 <- lm(log(CI) ~ Passage * Host,
    data = bacteria)

print(anova(M1))
```

YOu can see from this that we have a highly significant interaction between `Host` and `Passage` --- what this means is that the effect of `Passage` depends on whether the level of `Host` is `WT` or `myd88`, or alternatively, the effect of `Host` depends on whether `Passage` is `4` or `18`. Before we look into this further we should check the diagnostics for our model. Remember that you can bring up just the first two diagnostic plots with `which = 1:2`.

```{r prepare-M1, echo = FALSE}
M1 <- lm(log(CI) ~ Passage * Host,
    data = bacteria)
```

```{r bacteria_model1_diagnostics, exercise = TRUE, exercise.setup = "prepare-M1"}

```

```{r bacteria_model1_diagnostics-hint-1}
# Just use the plot() function on your model object
# and specify which = 1:2 as a second argument
```

```{r bacteria_model1_diagnostics-hint-2}
# This is the solution
plot(M1, which = 1:2)
```

What do the diagnostic plots tell us? More than one answer can be correct.

```{r diagnostics-quiz, echo=FALSE}
  question(
    "Which of the following is correct?",
    answer("Data points 44, 95 and 115 are ideintified as outliers and should be removed", message = "Answer 1. R will always identify the three residuals with the largest absolute values. That doesn't mean they are problematic and in this case there's no reason to be concerned.     "),
    answer("Because the points in the qq plot are all close to the line of unity we have a residual distribution which is very close to a normal distribution", correct = TRUE),
    answer("The systematic deviation from the line in the qq plot could be because the data are not independent", message = "Answer 3. The qq plot can't tell you anything about the independence of your data points, it only gives you information on how similar your residuals are to a normal distribution      "),
    answer("The plot of residuals versus fitted values indicates potentially important problems with heteroskedasticity", message = "Answer 4. The amount of dispersion in the residuals is roughly the same across the plot, indicating that there's no substantial change in the variance as the fitted values change      "),
    answer("The plot of residuals versus fitted values indicates potentially important problems with non-independence of data as shown by the data being divided into four groups", message = "Answer 5. The data are divided into four groups because the model is estimating four means      "),
    answer("The diagnostic plots are exemplary and give no reason for concern", correct = TRUE) 
)
```

## Interpreting the fitted model with two factors

We have a fitted model with a highly significant interaction term and we're happy that the diagnostic plots aren't showing us anything that gives any cause for concern. Next question: what does this mean?

The next thing we should do is bring up the summary for the model. Have a look and see if you can answer the questions. Remember that higher numbers for the competitiveness index mean that the experimentally evolved strain is more competitive.

```{r summary_M1, exercise=TRUE, exercise.setup = "prepare-M1"}

```

```{r summary_M1-hint-1}
# You just need to use the summary function
# on the fitted model object
```

```{r summary_M1-hint-2}
# This is the solution
summary(M1)
```


```{r summary_quiz1, echo = FALSE}
quiz(
caption = "Model summary quiz",
question("The line labelled (Intercept) gives the estimated mean value for what?",
       answer("WT Host and 4 Passages"),
       answer("WT Host and 18 Passages"),
       answer("myd88 host and 4 Passages", correct = TRUE),
       answer("myd99 hist and 18 Passages")
),

question("What is the estimated mean for myd88 Host and 18 Passages?",
         answer("3.4415"),
         answer("3.4415 + 0.4381", correct = TRUE),
         answer("3.4415 - 1.8047"),
         answer("3.4415 - 1.8047 + 2.9578")
),

question("Considering _only the myd88 treatments_, which statement is true?",
         answer("Bacteria passaged for 18 generations are signficantly more cometitive than those passaged for 4"),
         answer("Bacteria passaged for 18 generations are significantly less competitive than those passaged for 4"),
         answer("The competiveness of bacteria passaged for 18 generations is signficantly similar to that of bacteria passaged for 4 generations"),
         answer("Bacteria passaged for 18 generations are slightly more competitive than those passaged for 4, but we have little confidence that this pattern has arisen by anything other than random chance", correct = TRUE)
)
)
```


<details><summary>**Click here for more**></summary>
<br>
For the bacteria competed in the myd88- hosts the summary table tells us that there is little effect of the number of passages the bacteria went through in their experimental evolution: the estimated mean for 4 passages is similar to that for 18 and the marginal p-value for the row of the coefficients table in question is considerably greater than 0.05. So far all is fine, now what about the bacteria competing in the wild type (WT) hosts? This is where the interaction term shows itself.

Here's that coefficients table again:

```
Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)        3.4415     0.2957  11.637  < 2e-16 ***
Passage18          0.4381     0.4030   1.087    0.278    
HostWT            -1.8047     0.4182  -4.315 2.49e-05 ***
Passage18:HostWT   2.9578     0.5700   5.190 5.08e-07 ***
```
The `(Intercept)` is the estimated mean for myd88^-^ hosts and 4 passages.

The `Passage18` row gives the estimated change in the mean log CI associated with changing the bacteria from those that had 4 passages to those that had 18. If there were no interaction, as in the example that was worked through earlier in the tutorial, this would apply to all levels of the other factor. Because there is an interaction this applies to the myd88^-^ treatments but things get a bit more complicated with the WT treatments.

The `HostWT` row is the effect of changing `Host` to WT from myd88^-^, when the bacteria have been through 4 passages. You can see that this is a negative effect and that it is associated with a very small marginal p-value, meaning that we're reasonably confident that this isn't just something that arose through smapling error. The estimated mean log CI for WT hosts and 4 passages is therefore 3.4415 - 1.8047 =  1.6368.

Now, what about the mean for WT hosts and bacteria which have had 18 passages? If there were no interaction term this would be 3.4415 (intercept) + 0.4381 (effect of passage number) - 1.8047 (effect of host). It's not though, because we have an interaction term, so the effect of passage number depends on the host. The last row of the coefficients table, `Passage18:HostWT` gives us the effect of this interaction which is 2.9578. This is the difference in the effect of passage number when the host is WT as opposed to an myd88^-^ host: so the estimated mean for WT hosts and bacteria that have had 18 passages is (wait for it)

3.4415 (intercept) + 0.4381 (effect of passage number in myd88^-^ hosts) - 1.8047 (effect of host) + 2.9578 (difference in the effect of passage number depending on the host) = 5.0327.

We can check this against the actual mean:

```{r}
with(bacteria, mean(log(CI[Passage == "18" & Host == "WT"])))
```





```{r}
library(gplots)

# Plot means and 95% CIs. Note no x-axis (xaxt = "n") or x-axis label  
# (xlab = "") plotted, no lines connecting the means (connect = FALSE), 
# no sample size indicators (n.label = FALSE)
# and we are drawing the means and CIs larger and thicker
# than the defaults (cex = 2.5, barwidth = 2). 
plotmeans(
  log(CI) ~ interaction(Host:Passage),
  data = bacteria,
  n.label = FALSE,
  pch = 18,
  cex = 2.5,
  col = "darkblue",
  barcol = "darkblue",
  barwidth = 2,
  connect = FALSE,
  xlab = "",
  ylab = "Log Competitive Index",
  xaxt = "n",
  ylim = c(-3, 10)
)

# Use points function to draw in the data.
# The extra 88 at the end of the hex code for
# the colour makes the points semitransparent

# This horror show of nested parentheses:
# jitter(as.numeric(interaction(Treatment:Sex)), 0.6),
# uses the interaction() function to generate a factor
# with all the combinations of levels from the interaction
# of sex and treatment, then converts it to numerical
# values, then adds some noise with the jitter() function
# in order to reduce the amount of overplotting for the 
# individual data points in the final graph



points(
  log(CI) ~ jitter(as.numeric(interaction(Host:Passage)), 0.6),
  data = bacteria,
  pch = 16,
  col = "#00777755",
  cex = 1.2
)


# Draw in the axis, the axis ticks and the top line of
# the tick labels
axis(
  side = 1,
  at = 1:4,
  labels = c("4 Passages", "18 Passages", "4 Passages", "18 Passages")
)

# Use axis to draw in the "Female" and "Male" labels
# at the appropriate place. Line = 1.5 moves the axis down
# and lwd = 0 means the axis line is not drawn.
axis(
  side = 1,
  at = c(1.5, 3.5),
  line = 1.5,
  labels = c(expression('myd88' \u207B ), "Wild Type"),
  lwd = 0
)
```