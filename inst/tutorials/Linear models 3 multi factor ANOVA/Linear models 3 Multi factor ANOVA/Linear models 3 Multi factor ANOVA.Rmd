---
title: "Linear models 3: Multi-factor ANOVA"
output: 
  learnr::tutorial:
    progressive: false
    theme: cerulean
    highlight: textmate
    css: css/test.css
    code_folding: hide
runtime: shiny_prerendered
author: Rob Knell
description: >
  Analysis of variance with multiple explanatory factors: how to fit models, check diagnostics and interpret the output..
---

```{r setup, include=FALSE}
library(learnr)
library(gplots)
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE, fig.width = 5, fig.height = 5)
mouse_movement <-
  read.csv(
    "https://github.com/rjknell/Linear_models/raw/master/data/McCarthy_mice_activity.csv",
    stringsAsFactors = TRUE
  )
```

## Adding extra variables to a linear model

So far we've seen how the linear model can cope with both factors and continuous variables as explanatory variables, and that the way that variance is partitioned for both types of epxlanatory variable is fundamentally the same. Linear models can also cope with more than one explanatory variable, so you can analyse the effect of several different things that might influence your variable of interest at the same time. This has some important benefits over analysing each explanatory variable's effect separately:

* Fitting a single model reduces the number of statistical tests which we need to do, reducing the probability of type 1 errors.

* We can sometimes detect effects which we would not see in single variable analyses, because the linear model allows us to ask the question "what is the effect of variable Y when we have already taken the effect of variable X into account?".

* We can statistically control for variables if we wish to: so in a study with height and weight as explanatory variables we can ask the question "What is the effect of weight when height has been taken into account?" --- in other words, what is the effect of an individual being heavier or lighter than would be expected for someone of that height.

* We can look for *interactions* between explanatory variables which arise when the effect of one variable depends on the level of the other.

This video explains how the variance is partitioned when there is more than one explanatory factor and what some of the consequences of this are.

VIDEO

In this tutorial we'll look at analysing linear models with more than one factor as the explanatory variables. These linear models are often called "multi-factor ANOVA" or you might see them called a "two-way ANOVA" or similar depending on the number of explanatory factors used.

## Two-factor linear model example

The data we'll be looking at here come from a study on the effect that paternal exposure to nicotine has on the behaviour of their descendants, as published by McCarthy and co-authors in 2018^1^. The dataset we will use is a small subset of what was presented in their paper, and deals with  the spontaneous locomotor activity that the F1 offspring of nicotine exposed or control males mated with unexposed females exhibited over a 12 hour period. Locomotor activity was measured by placing the animals in a testing chamber with a series of infra-red beams arranged in a grid, and each time a beam was broken this was logged as a single locomotory event. Our dataset includes data from both male and female F1 offspring.

Let's load it from github and check its structure.

```{r}

mouse_movement <-
  read.csv(
    "https://github.com/rjknell/Linear_models/raw/master/data/McCarthy_mice_activity.csv",
    stringsAsFactors = TRUE
  )

str(mouse_movement)
```
Because we specified `stringsAsFactors = TRUE` as an argument both `Treatment` and `Sex`have been imported as factors. `SLA` (Spontaneous Locomotory Activity) is our response variable and is the count of all the recorded times an infra-red beam was broken during the 12-hour period. `Sex` is the sex of the F1 mouse in question, and `Treatment` is the treatment the father of the mouse was exposed to: "water" where the mouse was given plain drinking water and "nicotine" where the mouse was given drinking water containing 200Âµg/mL nicotine for 12 weeks. Let's look at our sample sizes for each factor combination.

```{r}
table(mouse_movement$Sex:mouse_movement$Treatment)
```

The data are somewhat unbalanced, with sample sizes ranging from 11 to 18 and overall rather more data for the controls whose fathers were given water but there's no combination with a really small or large sample size by comparison to the others. Now let's vsualise these data with a boxplot.

```{r fig.cap = "**Figure 1.** Boxplot showing Spontaneous Locomotor Activity (SLA) as measured in IR beam triggers over 12 hours for mice with paternal expsoure to either nicotine or water."}

boxplot(SLA ~  Treatment * Sex, 
        data = mouse_movement,
        xlab = "Treatment & Sex",
        ylab = "SLA(movements/12h)")
```

Looking at this we can see some patterns: spontaous movement rates seem more common in females and in animals with fathers exposed to nicotine, but as always we need to do some statistics to give us an idea of how confident we can be that thses patterns aren't just a consequence of sampling error. The boxplots are also at least approximately symmetrical which tells us that these data aren't strongly skewed. Ther do seem to be some differences in variance between the groups such that the mice with fathers exposed to nicotine have rather wider IQRs than those whose fathers were given plain water. It's not 100% clear how severe this is but we should keep an eye out for evidence of heterogeneous variances when we look at our diagnostic plots.

Fitting linear models with multiple explanatory variables in R simply involves adding new elements to the formula that is the first argument of the `lm()` function.

`SLA ~ Treatment` will fit a model with `Treatment` as the explanatory factor.

`SLA ~ Treatment + Sex` will fit a model with both `Treatment` and `Sex` as the explanatory factors but with no interaction between them --- in other words the *main effects* of the two factors only.

`SLA ~ Treatment + Sex + Treatment:Sex` will fit a model with both `Treatment` and `Sex` as explanatory factors but also with the interaction between the two (specified here by `Treatment:Sex`).

`SLA ~ Treatment * Sex` will, in this case, fit the same model as the previous example. The asterisk `*` means "fit all the main effects and also all the interactions. Here there can only be one interaction because there are only two explanatory factors. If there were a third factor, however, for example `Diet` (good or bad) then

`SLA ~ Treatment*Sex*Diet` would fit all three main effects, the two-way interactions between `Treatment` and `Sex`, between `Treatment` and `Diet` and between `Diet` and `Sex`, and also the three-way interaction between `Treatment`, `Sex` and `Diet`.

`SLA ~ Treatment + Sex + Diet + Treatment:Sex + Treatment:Diet` will fit all three main effects plus two interaction terms only, namely those between `Treatment` and `Sex` and between `Treatment` and our fictional `Diet` factor.

Let's fit a model with both main effects and the interaction, and then bring up an ANOVA table to get an idea of the statistical significance of our explanatory variables.

```{r eval = FALSE}
mouse1 <- lm(SLA ~ Treatment * Sex, data = mouse_movement)

anova(mouse1)
```

```{r echo = FALSE}
mouse1 <- lm(SLA ~ Treatment * Sex, data = mouse_movement)

print(anova(mouse1))
```

The basic structure here is familiar from the single factor ANOVA tutorial. Instead of simply partitioning the variance into that explained by the treatment and the remaining, error variance, however, we have now partitioned it into that explained by the two main effects of `Treatment` and `Sex`, plus a further sum of squares etc. for the interaction between the two and then the error variance (which R calls the `Residuals` of course). The various elements such as the Mean square values and the F-statistics are calculated in the same way as for any other linear model, so MS Sex  = SS Sex (253190901) divided by df Sex (1), and F for Sex is MS Sex/MS Residuals (or error) = 253190901/19714072 = 12.84316 on 1 and 50 df which is highly significant (p = 0.000767).

1: McCarthy, D.M., Morgan, T.J., Jr, Lowe, S.E., Williamson, M.J., Spencer, T.J., Biederman, J. & Bhide, P.G. (2018) Nicotine exposure of male mice produces behavioral impairment in multiple generations of descendants. PLoS biology, 16, e2006497.

## Model diagnostics

Before we go any further with interpreting this model, however, we should check our diagnostic plots to make sure that the assumptions that we make about our data are met, or at least are approximately met.

```{r}
plot(mouse1, which = 1:2)
```

The residuals versus fitted values plot shows little to concern us. The differences in variance between groups that we saw in the boxplot earlier aare there but overall these are not of the degree of magnitude that might cause us serious concern. The qq plot is a bit odd with some deviation from the line of equality but most of the deviations aren't too major. There are some datapoints that have high residual values (6, 48, 38) but again these are not enough to really cause a lot of concern. It's a bit of a judgement call in this case but it's most likely OK. We can do a further check by drawing a histogram of the residuals.

```{r}
hist(mouse1$residuals)
```

You can see from the histogram that the distribution of the residuals is a bit pointier than a normal distribution, with a big sharp peak in the centre of the distribution, and there's a suggestion of some positive skew. Overall though the distribution of the residuals is at least approximately normal and close to symmetrical. Given the general robustness of ANOVA to small to medium deviations from the fundamental assumptions about the data these fairly small departures from the ideal distribution are not likely to be affecting our output in any substantial way.

As a side note, this sort of error distribution is one of the main reasons why these tutorials are all using real data from real science. When you fit models to real data, there is *always* something about the diagnostic plots that makes you scratch your head. You never see residuals perfectly lined up in your qq plot, or with no pattern whatsoever in the residuals versus fitted values. The thing to ask yourself is whether whatever weirdness you're seeing is of a magnitude that might change the results of your analysis. If it is (and knowing what will and what won't is often a matter of experience) then you need to do something about it. If it isn't then don't. If you're not sure, then maybe try something (data transformation, removing highly influential and worrying data points), refit your model, compare it with the original and see if it makes a difference. If it doesn't then you know you're good.


## Significance of model terms

Now that we're reasonably happy about our diagnostics, let's look at the output of our analysis again. We've already seen that we have a non-significant interaction term but the two main effects are significant. How reliable are these p-values though? Take a look at what happens when we change the order of the explanatory variables in our model formula. Here's that first ANOVA table again.

```{r eval = FALSE}
# ANOVA table for first model
anova(mouse1)
```

```{r echo = FALSE}
print(anova(mouse1))
```

Here's the ANOVA table for the model with the explanatory variables in the opposite order.

```{r eval = FALSE}
# Fit model with Sex first instead of treatment
mouse2 <- lm(SLA ~ Sex *Treatment, data = mouse_movement)

# ANOVA table for second model
anova(mouse2)
```

```{r echo = FALSE}
# Fit model with Sex first instead of treatment
mouse2 <- lm(SLA ~ Sex *Treatment, data = mouse_movement)

# ANOVA table for second model
print(anova(mouse2))
```

The statistics for the two main effects change depending on whether their order in the formula. When treatment is first we have F~1,50~ = 15.44, p = 0.000262, but when treatment is the second factor in the model we have F~1,50~ = 13.79, p = 0.000516. This is a consequence of the way that a linear model is fitted: as explained in the video at the start of the tutorial the first term is based on the sum of squares etc. calculated from the raw data, but for the second term the sum of squares etc. is calculated using the residual sum of squares once the sum of squares for the first term has been removed. This method of calculating the sums of squares is called *type I sums of squares* or sometimes *sequential sums of squares*, and is why the SS, MS, F-statistic and p-value can change depending on the order of the terms in the model --- this will happen so long as there is any kind of correlation between the explanatory variables, and even when the explanatory variables are factors this will happen unless the design is perfectly balanced, with equal sample sizes for every combination of treatments.

Do we need to worry about this here? Maybe. One option is to use something called the *type III sums of squares* --- a different way of calculating the sums of squares which treats every term as if it were the last term entered into a model, so in this case the SS for Sex is calculated based on data which has already had the SS for Treatment and for the interaction partitioned out, and the SS for Treatment is calculated based on data which has already had the SS for Sex and for the interaction partitioned out. Some statistical software (e.g. SAS) uses these type III sums of squares by default, so if your analysis is giving a different result from one done with different software then this is worth bearing in mind as a possible explanation. There are also type II sums of squares but these only work in models where there is no interaction term.

To calculate the type III sums of squares we can use the `Anova()` function from the `car` package. Note that this function has a capital A to distinguish it clearly and straightforwardly from the `anova()` function we've already used. What could possibly go wrong?

```{r eval = FALSE}
library(car)

Anova(mouse1, type = "III")
```
```{r echo = FALSE}

library(car)

print(Anova(mouse1, type = "III"))
```

This supports what we've already calculated using the type I sums which are the default option in R: both main effects are significant but the interaction term is non-significant.

Since the interaction is very far from significant, and there's nothing to indicate that there is an important interaction effect here, we can consider generating a new and more parsimonious model without the interaction term. Ther is more in the tutorial on model selection on the reasoning behind this and when it might or might not be justified.

```{r eval = FALSE}
mouse3 <- lm(SLA ~ Treatment + Sex, data = mouse_movement)

Anova(mouse3, type = "III")
```

```{r echo = FALSE}
mouse3 <- lm(SLA ~ Treatment + Sex, data = mouse_movement)

print(Anova(mouse3, type = "III"))
```

Our main effects remain statistically significant in the new reduced model. Now that we have a fitted model and we're happy with our understanding of the signficance of the various terms, let's think about interpreting it.

## Interpreting the fitted model

Let's look at the `summary()` output for our model. You might want to have a look at the video on interpreting this output for ANOVA type models.

VIDEO

```{r}
summary(mouse3)
```

We have an ANOVA with two factors, each of which has two treatments. This gives us a fairly straightforward coefficients table. As with the single factor ANOVA models, the first row of the coefficients table is labelled `(Intercept)`. Instead of telling us the estimated mean for the treatment that comes first in the alphabet this is now the estimated mean for the treatment combination with the levels for each factor that are first alphabetically. Our factor levels are

```{r}
levels(mouse_movement$Treatment)
levels(mouse_movement$Sex)
```

So the intercept for this model is the estimated mean for female mice with fathers treated with nicotine. We can check this by calculating the mean directly.

```{r}
with(mouse_movement,
     mean(SLA[Sex == "Female" & Treatment == "Nicotine"]))
```

The estimate from the model is very close to the actual mean but not dead on. This is because we have no interaction in the model and consequently it is not estimating every single mean exactly: rather the intercept is estimated and then the effects of `Treatment` and `Sex`.

As with single factor ANOVA, we have a standard error, t-statistic and p-value for the intercept which tells us nothing more than this mean is significantly different from zero.

The next row is labelled `TreatmentWater`.

```
               Estimate Std. Error t value Pr(>|t|)  
TreatmentWater    -4545       1212  -3.750 0.000453 ***
```

 This coefficient gives the estimated difference between the mean for mice with paternal nicotine treatment (the intercept) and mice with paternal water treatment - mice whose fathers were treated with water cause 4545 fewer triggers per 12 hours. Because we have no interaction term in our model this applies to both sexes of mice equally: looking at this from the perspective of the nicotine treatment, both male and female mice whose fathers were treated with nicotine cause, on average, 4545 extra triggers per night. In other words, paternal nicotine treatment is associated with a considerable increase in spontaneous locomotor activity --- almost a third more in fact. The standard error, t-test and the small marginal p-value for this row of the coefficients table tells us that an effect this big, or bigger, is unlikely to have arisen simply by sampling error.

The last line is labelled `SexMale`.

```
               Estimate Std. Error t value Pr(>|t|)  
SexMale           -4365       1206  -3.619 0.000678 ***
```

Much as the previous line gives us the effect of paternal exposure, this coefficient gives us the effect of sex. The intercept was for females, and since the estimate for males is -4365 this tells us that on average males will cause 4365 fewer triggers per 12 hours, whether or not their fathers were exposed to nicotine.

Because we have no signficant interaction between our explanatory factors, their effects are not dependent on the value of the other explanatory factor. In other words, the effects of each factor are *additive* in that to get the model prediction for a particular combination of factor levels we can simply add the relevant effects together. So for a male with paternal exposure to nicotine the predicted value is 

Intercept (14554) + the coefficient for Sex (-4365) = 10189, 

whereas for a male with paternal expsore to water the predicted value is

Intercept (14554) + the coefficient for Sex (-4365) + the coefficient for Treatment (-4545) = 5644.

Do we need to go any further with interpreting this model, for example by doing a post-hoc test to assess exactly which means are different from which other means? No. We know the effects of both `Sex` and `Treatment` and we can express those in terms of the actual behaviour of these mice. We know that these main effects are statistically highly significant, and we know that the interaction term is far from significant and can safely be ignored. Further post-hoc testing would not really be helpful.

Let's finish by plotting out our means and confidence intervals. This is a fairly complicated piece of code because we're also plotting the data over the top, and drawing the x-axis labels in using the `axis()` function twice, once for each line.

```{r fig.cap = "**Figure 2.** Spontaneous Locomotor Activity (SLA) as measured in IR beam triggers over 12 hours for mice with paternal expsoure to either nicotine or water. Diamonds indicate means and error bars are 95% confidence intervals."}
# Load the gplots package so we can use 
# the plotmeans() function 
library(gplots)

# Plot means and 95% CIs. Note no x-axis (xaxt = "n") or x-axis label  
# (xlab = "") plotted, no lines connecting the means (connect = FALSE), 
# no sample size indicators (n.label = FALSE)
# and we are drawing the means and CIs larger and thicker
# than the defaults (cex = 2.5, barwidth = 2). 
plotmeans(
  SLA ~ interaction(Treatment:Sex),
  data = mouse_movement,
  n.label = FALSE,
  pch = 18,
  cex = 2.5,
  col = "darkblue",
  barcol = "darkblue",
  barwidth = 2,
  connect = FALSE,
  xlab = "",
  ylab = "SLA (triggers per 12 hours)",
  xaxt = "n",
  ylim = c(0, 24000)
)

# Use points function to draw in the data.
# The extra 88 at the end of the hex code for
# the colour makes the points semitransparent

# This horror show of nested parentheses:
# jitter(as.numeric(interaction(Treatment:Sex)), 0.6),
# uses the interaction() function to generate a factor
# with all the combinations of levels from the interaction
# of sex and treatment, then converts it to numerical
# values, then adds some noise with the jitter() function
# in order to reduce the amount of overplotting for the 
# individual data points in the final graph



points(
  SLA ~ jitter(as.numeric(interaction(Treatment:Sex)), 0.6),
  data = mouse_movement,
  pch = 16,
  col = "#00777788",
  cex = 1.2
)


# Draw in the axis, the axis ticks and the top line of
# the tick labels
axis(
  side = 1,
  at = 1:4,
  labels = c("Nicotine", "Water", "Nicotine", "Water")
)

# Use axis to draw in the "Female" and "Male" labels
# at the appropriate place. Line = 1.5 moves the axis down
# and lwd = 0 means the axis line is not drawn.
axis(
  side = 1,
  at = c(1.5, 3.5),
  line = 1.5,
  labels = c("Female", "Male"),
  lwd = 0
)
```


## Exercise: multi-factor ANOVA 

Here, we will use some data from a study on bacterial adaptation to host gut environments originally published in 2018 by a group of researchers at The UNiversity of Eugene, Oregon and the Canadian Institute for Advanced Research~2~. They looked, among other things, at the competitive ability a bacterium called *Aeromonas veronii* after either 4 or 18 passages through the gut of an otherwise germ-free larval zebrafish. Competitive ability was measured *in vivo* by incoulating a larval zebrafish with a dose of both the experimental line of bacteria and an ancestral ine which was tagged with Green Flourescent Protein for 3 days, following which the gut contents were homogenised and the homogenate plated onto tryptone soy agar. After a suitable incubation period the colonies of each strain were counted using a flourescence microscope to distinguish them and a "competitive index" calclated as the ratio of the number of colonies for the adapted line to the colonies from the ancestral line.

The particular experiment which we will look at here is one in which the experimenters compared the competitive ability of bacteria in the host strain which they had been evolving (WT hosts) with the competitive ability of the same bacteria but competing in hosts of a different genetic background ()  compared the competitive abilities of bacteria which had had a short period of adaptation (4 passages) with bacteria which had a longer period (18 passages), 

2. Robinson, C.D., Klein, H.S., Murphy, K.D., Parthasarathy, R., Guillemin, K. & Bohannan, B.J.M. (2018) Experimental bacterial adaptation to the zebrafish gut reveals a primary role for immigration. PLoS biology, 16, e2006893.

